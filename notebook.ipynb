{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Advanced Data Analytics - Algorithms and Machine Learning\n",
    "## 31005\n",
    "### Harrison Cole\n",
    "### 12962712"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Section 1 - Imports\n",
    "Imports libraries and type-definitions for use throughout the program."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 829,
   "outputs": [],
   "source": [
    "import abc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import traceback\n",
    "\n",
    "from typing import Callable, Optional, Tuple, List, Dict\n",
    "from sklearn.datasets import load_iris as dataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Section 2 - Utility Function Definitions\n",
    "Defines utility functions for (re)use throughout the program."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 830,
   "outputs": [],
   "source": [
    "def require(value: Optional[any], field: str) -> any:\n",
    "    if value is None:\n",
    "        raise ValueError(f'Missing required value: \"{field}\".')\n",
    "    return value\n",
    "\n",
    "\n",
    "def default(value: Optional[any], otherwise: any) -> any:\n",
    "    return otherwise if value is None else value\n",
    "\n",
    "\n",
    "def value_counts(y, normalise: bool = True):\n",
    "    values, counts = np.unique(y, return_counts=True)\n",
    "    if normalise:\n",
    "        return values, counts / np.sum(counts)\n",
    "    return values, counts\n",
    "\n",
    "def majority_class_index(data, attribute):\n",
    "    return np.argmax(np.unique(data[attribute], return_counts=True)[1])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Section 3 - Data-structures, Interfaces and Implementations\n",
    "Defines the API and data-structures available for use throughout this program. Where applicable, effort is taken\n",
    "to program by contract against the interface rather than the implementation."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Section 3.1 - Split Criterion Metrics"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class SplitCriterionMetric(metaclass=abc.ABCMeta):\n",
    "    \"\"\"\n",
    "    An interface for computing the measure of quality produced by splitting the set of items across\n",
    "    the axis of a given variable at each step of computation during the tree building process.\n",
    "    \"\"\"\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def compute(self, frequencies) -> float:\n",
    "        \"\"\"\n",
    "        Computes a measure of quality, usually the homogeneity of the target class, represented by the\n",
    "        frequences of each target class instance within a subset of the dataset.\n",
    "        :param frequencies:\n",
    "        The frequencies of each target class instance within this subset.\n",
    "        :return:\n",
    "        A value between 0 and 1 where higher values indicate a higher degree of homogeneity.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class Entropy(SplitCriterionMetric):\n",
    "\n",
    "    def compute(self, frequencies) -> float:\n",
    "        \"\"\"\n",
    "        Computes the entropy of the target class within a subset of the dataset.\n",
    "        :param frequencies:\n",
    "        The frequencies of each class instance within this subset.\n",
    "        :return:\n",
    "        A measure of the randomness of the distribution of each target class instance within this subset.\n",
    "        \"\"\"\n",
    "        eps=1e-9\n",
    "        return -(frequencies * np.log2(frequencies + eps)).sum()\n",
    "\n",
    "\n",
    "class GiniImpurity(SplitCriterionMetric):\n",
    "\n",
    "    def compute(self, frequencies) -> float:\n",
    "        \"\"\"\n",
    "        Computes the Gini impurity of the target class within a subset of the dataset.\n",
    "        :param frequencies:\n",
    "        The frequencies of each class instance within this subset.\n",
    "        :return:\n",
    "        A measure of how often a randomly chosen element from the dataset would be incorrectly labelled if\n",
    "        it was labelled according to the distribution of class instances within this subset.\n",
    "        \"\"\"\n",
    "        return 1 - np.sum(np.square(frequencies))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Section 3.2 - Pivot"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Pivot:\n",
    "    \"\"\"\n",
    "    TODO: documentation\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, predicate: Callable[[any], bool], info: Tuple[any, any, str, str]):\n",
    "        self.__predicate = require(predicate, 'predicate')\n",
    "        self.__info = require(info, 'info')\n",
    "\n",
    "    @property\n",
    "    def predicate(self) -> Callable[[any], bool]:\n",
    "        return self.__predicate\n",
    "\n",
    "    def attribute(self) -> any:\n",
    "        \"\"\"\n",
    "        The value of the variable being pivoted upon.\n",
    "        \"\"\"\n",
    "        return self.__info[0]\n",
    "\n",
    "    def point(self) -> any:\n",
    "        \"\"\"\n",
    "        The value(s) of the pivot point.\n",
    "        \"\"\"\n",
    "        return self.__info[1]\n",
    "\n",
    "    def true_condition(self) -> str:\n",
    "        \"\"\"\n",
    "        The affirmative textual representation of the predicate.\n",
    "        \"\"\"\n",
    "        return self.__info[2]\n",
    "\n",
    "    def false_condition(self) -> str:\n",
    "        \"\"\"\n",
    "        The negative textual representation of the predicate.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return self.__info[3]\n",
    "\n",
    "    def split(self, value: any) -> bool:\n",
    "        return self.predicate(value)\n",
    "\n",
    "    # TODO: rename method as this is only used for splits on continuous attributes...\n",
    "    @staticmethod\n",
    "    def continuous(attribute, point) -> 'Pivot':\n",
    "        def predicate(value: any) -> bool:\n",
    "            return value[attribute] <= point\n",
    "        return Pivot(predicate=predicate, info=(attribute, point, '<=', '>'))\n",
    "\n",
    "    def __str__(self, condition: bool = True) -> str:\n",
    "        operator: str = self.true_condition() if condition else self.false_condition()\n",
    "        return f'x[{self.attribute()}] {operator.ljust(2)} {self.point()}'\n",
    "\n",
    "\n",
    "class PivotCandidate:\n",
    "    \"\"\"\n",
    "    TODO: documentation\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, feature: any, gain: float, probe: float):\n",
    "        self.__feature = feature\n",
    "        self.__gain = gain\n",
    "        self.__probe = probe\n",
    "\n",
    "    def feature(self) -> any:\n",
    "        return require(self.__feature, 'feature')\n",
    "\n",
    "    def gain(self) -> float:\n",
    "        return require(self.__gain, 'gain')\n",
    "\n",
    "    def probe(self) -> float:\n",
    "        return require(self.__probe, 'probe')\n",
    "\n",
    "    def update(self, feature: int, gain: float, probe: float) -> bool:\n",
    "        if gain < self.gain():\n",
    "            return False\n",
    "        self.__feature = feature\n",
    "        self.__gain = gain\n",
    "        self.__probe = probe\n",
    "        return True\n",
    "\n",
    "    @staticmethod\n",
    "    def initial() -> 'PivotCandidate':\n",
    "        return PivotCandidate(0, 0, 0.5)\n",
    "\n",
    "    def __str__(self):\n",
    "        return f'feature: {self.__feature}, gain: {self.__gain}, probe: {self.__probe}'\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 833,
   "outputs": [],
   "source": [
    "class Node(metaclass=abc.ABCMeta):\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def eval(self, element: any) -> any:\n",
    "        raise NotImplementedError('Node#eval')\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def depth(self, level: int = 0) -> int:\n",
    "        pass\n",
    "\n",
    "    def prune(self) -> 'Node':\n",
    "        return self\n",
    "\n",
    "    @staticmethod\n",
    "    def terminate(value: any) -> 'Node':\n",
    "        return TerminalNode(value=value)\n",
    "\n",
    "    @staticmethod\n",
    "    def branch(pivot: 'Pivot', lower: Optional['Node'] = None, upper: Optional['Node'] = None) -> 'Node':\n",
    "        return BranchNode(pivot=pivot, lower=lower, upper=upper)\n",
    "\n",
    "    @staticmethod\n",
    "    def lookup(mapping: Dict[any, 'Node'], feature: any) -> 'Node':\n",
    "        return LookupNode(mapping=mapping, feature=feature)\n",
    "\n",
    "\n",
    "class TerminalNode(Node):\n",
    "\n",
    "    def __init__(self, value: any):\n",
    "        self.__value = require(value, 'value')\n",
    "\n",
    "    def eval(self, element: any) -> any:\n",
    "        return self.value\n",
    "\n",
    "    def depth(self, level: int = 0) -> int:\n",
    "        return level\n",
    "\n",
    "    @property\n",
    "    def value(self) -> any:\n",
    "        return require(self.__value, 'value')\n",
    "\n",
    "\n",
    "class BranchNode(Node):\n",
    "\n",
    "    def __init__(self, pivot: 'Pivot', lower: 'Node', upper: 'Node'):\n",
    "        self.__pivot = require(pivot, 'pivot')\n",
    "        self.__lower = require(lower, 'lower')\n",
    "        self.__upper = require(upper, 'upper')\n",
    "\n",
    "    def eval(self, element: any) -> any:\n",
    "        branch: Node = self.lower if self.pivot.split(element) else self.upper\n",
    "        return branch.eval(element)\n",
    "\n",
    "    def depth(self, level: int = 0) -> int:\n",
    "        return max(self.lower.depth(level=level + 1), self.upper.depth(level=level + 1))\n",
    "\n",
    "    def prune(self) -> 'Node':\n",
    "        if isinstance(self.lower, TerminalNode) and isinstance(self.upper, TerminalNode) and self.lower.value == self.upper.value:\n",
    "            return self.lower\n",
    "        return self\n",
    "\n",
    "    @property\n",
    "    def pivot(self) -> 'Pivot':\n",
    "        return self.__pivot\n",
    "\n",
    "    @property\n",
    "    def lower(self) -> 'Node':\n",
    "        return self.__lower\n",
    "\n",
    "    @property\n",
    "    def upper(self) -> 'Node':\n",
    "        return self.__upper\n",
    "\n",
    "\n",
    "class LookupNode(Node):\n",
    "\n",
    "    __mapping: Dict[any, 'Node']\n",
    "    __feature: any\n",
    "\n",
    "    def __init__(self, mapping: Dict[any, 'Node'], feature: any):\n",
    "        self.__mapping = require(mapping, 'mapping')\n",
    "        self.__feature = require(feature, 'feature')\n",
    "\n",
    "    def eval(self, element: any) -> any:\n",
    "        value = element[self.feature]\n",
    "        lookup: Node = require(self.mapping[value], 'lookup')\n",
    "        return lookup.eval(element)\n",
    "\n",
    "    def depth(self, level: int = 0) -> int:\n",
    "        return max([node.depth(level=level + 1) for node in self.mapping.values()], default=level + 1)\n",
    "\n",
    "    def prune(self) -> 'Node':\n",
    "        size = len(self.mapping)\n",
    "        if size == 1:\n",
    "            return next(iter(self.mapping.values()))\n",
    "        return self\n",
    "\n",
    "    @property\n",
    "    def mapping(self) -> Dict[any, 'Node']:\n",
    "        return self.__mapping\n",
    "\n",
    "    @property\n",
    "    def feature(self) -> any:\n",
    "        return self.__feature\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 834,
   "outputs": [],
   "source": [
    "entropy: SplitCriterionMetric = Entropy()\n",
    "gini: SplitCriterionMetric = GiniImpurity()\n",
    "\n",
    "class DecisionTreeBuilder(metaclass=abc.ABCMeta):\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def build(self, x, y) -> 'Node':\n",
    "        raise NotImplementedError('DecisionTreeBuilder#build')\n",
    "\n",
    "    def entropy(self, attributes, criterion: SplitCriterionMetric = entropy) -> float:\n",
    "        _, probabilities = value_counts(attributes, normalise=True)\n",
    "        return criterion.compute(frequencies=probabilities)\n",
    "\n",
    "    def information_gain_categorical(self, data, target_attribute, feature_attribute):\n",
    "        total_entropy = self.entropy(data[target_attribute])\n",
    "        feature_entropy = self.entropy(data[feature_attribute])\n",
    "        return total_entropy - feature_entropy\n",
    "\n",
    "    def information_gain_continuous(self, data, target_attribute, feature_attribute, probe):\n",
    "        size, target, feature = len(data), data[target_attribute], data[feature_attribute]\n",
    "        total_entropy = self.entropy(target)\n",
    "        lte, gt = feature <= probe, feature > probe\n",
    "\n",
    "        lower_entropy = self.entropy(target[lte]) * (np.count_nonzero(lte) / size)\n",
    "        upper_entropy = self.entropy(target[gt]) * (np.count_nonzero(gt) / size)\n",
    "\n",
    "        return total_entropy - (lower_entropy + upper_entropy)\n",
    "\n",
    "    @staticmethod\n",
    "    def factory(implementation: str, **kwargs) -> 'DecisionTreeBuilder':\n",
    "        factories = {\n",
    "            'ID3': ID3DecisionTreeBuilder\n",
    "        }\n",
    "        constructor = require(factories.get(implementation, None), implementation)\n",
    "        return constructor(**kwargs)\n",
    "\n",
    "    @staticmethod\n",
    "    def default() -> 'DecisionTreeBuilder':\n",
    "        return DecisionTreeBuilder.factory('ID3')\n",
    "\n",
    "\n",
    "class ID3DecisionTreeBuilder(DecisionTreeBuilder):\n",
    "\n",
    "    # TODO: handle continuous attributes\n",
    "    # TODO: output classes in original format...\n",
    "    # TODO: mean value\n",
    "    def build(self, x, y) -> 'Node':\n",
    "        data = x.copy()\n",
    "        data[y.name] = y\n",
    "        return self._build(original=data, subset=data, features=x.columns, target=y.name)\n",
    "\n",
    "    def _build(self, original, subset, features, target, parent_class=None) -> 'Node':\n",
    "        \"\"\"\n",
    "        ID3 Algorithm as per: https://en.wikipedia.org/wiki/ID3_algorithm#Algorithm\n",
    "        \"\"\"\n",
    "        classes = np.unique(subset[target])\n",
    "        choices = len(classes)\n",
    "\n",
    "        # base case #1\n",
    "        # Every element of the subset belongs to the same class.\n",
    "        if choices <= 1:\n",
    "            return Node.terminate(classes[0])\n",
    "\n",
    "        # base case #2\n",
    "        # There are no examples in the subset, which happens when no example in the parent set was found to match a\n",
    "        # specific value of the selected attribute\n",
    "        if len(subset) <= 0:\n",
    "            return Node.terminate(parent_class)\n",
    "\n",
    "        majority_class = classes[majority_class_index(data=subset, attribute=target)]\n",
    "\n",
    "        # base case #3\n",
    "        # There are no more attributes to be selected, but the examples still do not belong to the same class.\n",
    "        if len(features) <= 0:\n",
    "            return Node.terminate(majority_class)\n",
    "\n",
    "        gains = np.asarray([self.information_gain_categorical(data=subset, target_attribute=target, feature_attribute=feature) for feature in features])\n",
    "\n",
    "        best_feature = features[np.argmax(gains)]\n",
    "        attribute = subset[best_feature]\n",
    "\n",
    "        remaining_features = [feature for feature in features if feature != best_feature]\n",
    "\n",
    "        subtree: Node\n",
    "\n",
    "        # noinspection PyBroadException\n",
    "        try:\n",
    "            subtree = self._build_continuous(original=original, subset=subset, features=remaining_features, target=target, parent_class=majority_class, best_feature=best_feature, attribute=attribute)\n",
    "        except:\n",
    "            subtree = self._build_categorical(original=original, subset=subset, features=remaining_features, target=target, parent_class=majority_class, best_feature=best_feature, attribute=attribute)\n",
    "\n",
    "        while (pruned := subtree.prune()) != subtree:\n",
    "            subtree = pruned\n",
    "\n",
    "        return subtree\n",
    "\n",
    "    def _build_continuous(self, original, subset, features, target, parent_class, best_feature, attribute) -> 'Node':\n",
    "        probes = self.create_probe_values(attribute.min(), attribute.max())\n",
    "        candidate: PivotCandidate = PivotCandidate.initial()\n",
    "        for probe in probes:\n",
    "            gain = self.information_gain_continuous(data=subset, target_attribute=target, feature_attribute=best_feature, probe=probe)\n",
    "            candidate.update(feature=best_feature, gain=gain, probe=probe)\n",
    "\n",
    "        def build_subtree(indices) -> 'Node':\n",
    "            return self._build(original=original, subset=subset[indices], features=features, target=target, parent_class=parent_class)\n",
    "\n",
    "        pivot: Pivot = Pivot.continuous(candidate.feature(), candidate.probe())\n",
    "        lower = build_subtree(subset[candidate.feature()] <= candidate.probe())\n",
    "        upper = build_subtree(subset[candidate.feature()] > candidate.probe())\n",
    "\n",
    "        return Node.branch(pivot=pivot, lower=lower, upper=upper)\n",
    "\n",
    "    def _build_categorical(self, original, subset, features, target, parent_class, best_feature, attribute) -> 'Node':\n",
    "        values = np.unique(attribute)\n",
    "        mapping: Dict[any, Node] = {}\n",
    "\n",
    "        for value in values:\n",
    "            data = subset.where(subset[best_feature] == value).dropna()\n",
    "            subtree = self._build(original=original, subset=data, features=features, target=target, parent_class=parent_class)\n",
    "            mapping[value] = subtree\n",
    "\n",
    "        return Node.lookup(mapping=mapping, feature=best_feature)\n",
    "\n",
    "    # def _continuous(self) -> 'Node':\n",
    "    #     candidate: PivotCandidate = PivotCandidate.initial()\n",
    "    #     attributes = x.shape[1]\n",
    "    #     for index in range(attributes):\n",
    "    #         # TODO: handle continuous and categorical attributes\n",
    "    #         attribute = x[:, index]  # array of all values at that index\n",
    "    #\n",
    "    #         print(f'attribute => {attribute}')\n",
    "    #         # print(f'build')\n",
    "    #         # print(f'index: {index}, attribute: {attribute}')\n",
    "    #         probes = ID3DecisionTreeBuilder.create_probe_values(attribute.min(), attribute.max())\n",
    "    #         # print(f'probes: {probes}')\n",
    "    #         for probe in probes:\n",
    "    #             gain = ID3DecisionTreeBuilder.compute_information_gain(y, attribute, probe)\n",
    "    #             # gain = np.random.random()\n",
    "    #\n",
    "    #             # gain = 0.0  # compute_gain(samples, attribute, target)\n",
    "    #             # gain = self.purity(attribute, probe, x, y)  # TODO: compute information gain\n",
    "    #             # gain = self.measure_progress(y, attribute, probe)\n",
    "    #             # gain = self.purity(attribute, )\n",
    "    #             candidate.update(feature=index, gain=gain, probe=probe)\n",
    "    #\n",
    "    #     # TODO: sanity check candidate or build pivot from candidate\n",
    "    #     pivot: Pivot = Pivot.continuous(candidate.feature(), candidate.probe())\n",
    "    #     # TODO: use or apply pivot data-structure and make more efficient...\n",
    "    #     idx_lower = x[:, candidate.feature()] <= candidate.probe()\n",
    "    #     idx_upper = x[:, candidate.feature()] > candidate.probe()\n",
    "    #\n",
    "    #     def build_index(indices) -> Node:\n",
    "    #         return self.build(x[indices], y[indices])\n",
    "    #\n",
    "    #     return Node.branch(pivot, build_index(idx_lower), build_index(idx_upper))\n",
    "    #     pass\n",
    "\n",
    "    # def _categorical(self) -> 'Node':\n",
    "        # if is_categorical(subset[best_feature]):\n",
    "        #     print(f'{best_feature} IS CATEGORICAL')\n",
    "        # else:\n",
    "        #     print(f'{best_feature} IS NOT CATEGORICAL')\n",
    "        #\n",
    "        # # print(f'INFORMATION GAIN\\n{information_gain}\\n {best_feature_index} - {best_feature}')\n",
    "        #\n",
    "        # available_features = [feature for feature in features if feature != best_feature]\n",
    "        # values = np.unique(subset[best_feature])\n",
    "        #\n",
    "        # mapping: Dict[any, Node] = {}\n",
    "        #\n",
    "        # for value in values:\n",
    "        #     data = subset.where(subset[best_feature] == value).dropna()\n",
    "        #     subtree = self._build(original=original, subset=data, features=available_features, target=target, parent_class=majority_class)\n",
    "        #     mapping[value] = subtree\n",
    "        #\n",
    "        # return Node.lookup(mapping=mapping, feature=best_feature)\n",
    "        # pass\n",
    "\n",
    "    # # TODO: handle categorical attributes...\n",
    "    # def build(self, x, y) -> 'Node':\n",
    "    #     classes = np.unique(y)\n",
    "    #     choices = len(classes)\n",
    "    #\n",
    "    #     if choices <= 0:  # edge-case: no choices\n",
    "    #         default_value = '<todo:default-value>'  # TODO: get default value\n",
    "    #         return Node.terminate(default_value)\n",
    "    #\n",
    "    #     if choices == 1:  # edge-case: one clear choice\n",
    "    #         return Node.terminate(classes[0])\n",
    "    #\n",
    "    #     candidate: PivotCandidate = PivotCandidate.initial()\n",
    "    #     attributes = x.shape[1]\n",
    "    #     for index in range(attributes):\n",
    "    #         # TODO: handle continuous and categorical attributes\n",
    "    #         attribute = x[:, index]  # array of all values at that index\n",
    "    #\n",
    "    #         print(f'attribute => {attribute}')\n",
    "    #         # print(f'build')\n",
    "    #         # print(f'index: {index}, attribute: {attribute}')\n",
    "    #         probes = ID3DecisionTreeBuilder.create_probe_values(attribute.min(), attribute.max())\n",
    "    #         # print(f'probes: {probes}')\n",
    "    #         for probe in probes:\n",
    "    #             gain = ID3DecisionTreeBuilder.compute_information_gain(y, attribute, probe)\n",
    "    #             # gain = np.random.random()\n",
    "    #\n",
    "    #             # gain = 0.0  # compute_gain(samples, attribute, target)\n",
    "    #             # gain = self.purity(attribute, probe, x, y)  # TODO: compute information gain\n",
    "    #             # gain = self.measure_progress(y, attribute, probe)\n",
    "    #             # gain = self.purity(attribute, )\n",
    "    #             candidate.update(feature=index, gain=gain, probe=probe)\n",
    "    #\n",
    "    #     # TODO: sanity check candidate or build pivot from candidate\n",
    "    #     pivot: Pivot = Pivot.continuous(candidate.feature(), candidate.probe())\n",
    "    #     # TODO: use or apply pivot data-structure and make more efficient...\n",
    "    #     idx_lower = x[:, candidate.feature()] <= candidate.probe()\n",
    "    #     idx_upper = x[:, candidate.feature()] > candidate.probe()\n",
    "    #\n",
    "    #     def build_index(indices) -> Node:\n",
    "    #         return self.build(x[indices], y[indices])\n",
    "    #\n",
    "    #     return Node.branch(pivot, build_index(idx_lower), build_index(idx_upper))\n",
    "    #\n",
    "    # @staticmethod\n",
    "    # def measure_progress(y, attribute, target, criterion: str = 'entropy'):\n",
    "    #     size = len(y)\n",
    "    #     lte, gt = attribute <= target, attribute > target\n",
    "    #     total_e = DecisionTreeBuilder.compute_impurity(y, criterion=criterion)\n",
    "    #     lower_e = DecisionTreeBuilder.compute_impurity(y[lte], criterion=criterion)\n",
    "    #     upper_e = DecisionTreeBuilder.compute_impurity(y[gt], criterion=criterion)\n",
    "    #     lower_w = np.count_nonzero(lte) / size\n",
    "    #     upper_w = np.count_nonzero(gt) / size\n",
    "    #\n",
    "    #     return total_e - (lower_w * lower_e + upper_w * upper_e)\n",
    "    #\n",
    "    # @staticmethod\n",
    "    # def compute_information_gain(samples, attribute, target) -> float:\n",
    "    #     return ID3DecisionTreeBuilder.measure_progress(samples, attribute, target)\n",
    "    #     # classes, frequencies = value_counts(samples, normalise=True)\n",
    "    #     # total: float = DecisionTreeBuilder.compute_impurity(samples=target)\n",
    "    #     # cumulative: float = 0\n",
    "    #     # print(f'compute_information_gain')\n",
    "    #     # print(f'total: {total}')\n",
    "    #     # print(f'classes: {classes}')\n",
    "    #     # print(f'frequencies: {frequencies}')\n",
    "    #     # print(f'samples: {samples}')\n",
    "    #     # print(f'attribute: {attribute}')\n",
    "    #     # print(f'target: {target}')\n",
    "    #     # print()\n",
    "    #     # print()\n",
    "    #     # for (value, frequency) in zip(classes, frequencies):\n",
    "    #     #     print(f'class: {value}, frequency: {frequency}')\n",
    "    #     #     indices = attribute[attribute <= target]\n",
    "    #     #     # indices = [0]\n",
    "    #     #     # indices = attributes[]\n",
    "    #     #     # indices = samples[attribute] == value\n",
    "    #     #     # indices = samples[attribute == value]\n",
    "    #     #     print(f'indices {indices}')\n",
    "    #     #     # print(f'indices: {indices}')\n",
    "    #     #     contribution = DecisionTreeBuilder.compute_impurity(target[indices])\n",
    "    #     #     cumulative += frequency * contribution\n",
    "    #     # return total - cumulative\n",
    "    #\n",
    "\n",
    "    def create_probe_values(self, minima, maxima):\n",
    "        return [v * minima + (1.0 - v) * maxima for v in [0.75, 0.5, 0.25]]  # TODO: expand values\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 835,
   "outputs": [],
   "source": [
    "class Model(metaclass=abc.ABCMeta):\n",
    "\n",
    "    def compile(self, *args, **kwargs):\n",
    "        pass\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def fit(self, x, y, *args, **kwargs):\n",
    "        raise NotImplementedError('Model#fit')\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def predict(self, x, *args, **kwargs):\n",
    "        raise NotImplementedError('Model#predict')\n",
    "\n",
    "\n",
    "class DecisionTree(Model):\n",
    "\n",
    "    # TODO: default value (most common class..?)\n",
    "    __builder: 'DecisionTreeBuilder' = DecisionTreeBuilder.default()\n",
    "    __root: Optional['Node'] = None\n",
    "\n",
    "    def compile(self, *args, **kwargs):\n",
    "        previous: DecisionTreeBuilder = self.__builder\n",
    "        try:\n",
    "            builder = DecisionTreeBuilder.factory(kwargs['implementation'], **kwargs)\n",
    "        except (KeyError, ValueError):\n",
    "            builder = previous\n",
    "        self.__builder = builder\n",
    "\n",
    "    def fit(self, x, y, *args, **kwargs):\n",
    "        self.__root = self.builder.build(x, y)\n",
    "\n",
    "    def predict(self, x, *args, **kwargs):\n",
    "        tree: Node = self.root\n",
    "        samples = x.to_dict(orient='records')\n",
    "        return np.asarray([tree.eval(sample) for sample in samples])\n",
    "\n",
    "    @property\n",
    "    def builder(self) -> 'DecisionTreeBuilder':\n",
    "        return require(self.__builder, 'builder')\n",
    "\n",
    "    @property\n",
    "    def root(self) -> 'Node':\n",
    "        return require(self.__root, 'root')\n",
    "\n",
    "    def depth(self) -> int:\n",
    "        return 0 if self.root is None else self.root.depth()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Section 4 - Implementation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 836,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data\n",
      "     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
      "0                  5.1               3.5                1.4               0.2\n",
      "1                  4.9               3.0                1.4               0.2\n",
      "2                  4.7               3.2                1.3               0.2\n",
      "3                  4.6               3.1                1.5               0.2\n",
      "4                  5.0               3.6                1.4               0.2\n",
      "..                 ...               ...                ...               ...\n",
      "145                6.7               3.0                5.2               2.3\n",
      "146                6.3               2.5                5.0               1.9\n",
      "147                6.5               3.0                5.2               2.0\n",
      "148                6.2               3.4                5.4               2.3\n",
      "149                5.9               3.0                5.1               1.8\n",
      "\n",
      "[150 rows x 4 columns]\n",
      "\n",
      "Targets\n",
      "0      0\n",
      "1      0\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "      ..\n",
      "145    2\n",
      "146    2\n",
      "147    2\n",
      "148    2\n",
      "149    2\n",
      "Name: target, Length: 150, dtype: int64\n",
      "\n",
      "MAX DEPTH: 4\n",
      "1  |--  lower pivot: x[sepal width (cm)] <= 3.2\n",
      "2  |----  lower pivot: x[petal width (cm)] <= 0.675\n",
      "3  |------  class: 0\n",
      "2  |----  upper pivot: x[petal width (cm)] >  0.675\n",
      "3  |------  lower pivot: x[sepal length (cm)] <= 7.0\n",
      "4  |--------  lower pivot: x[petal length (cm)] <= 4.45\n",
      "5  |----------  class: 1\n",
      "4  |--------  upper pivot: x[petal length (cm)] >  4.45\n",
      "5  |----------  class: 2\n",
      "3  |------  upper pivot: x[sepal length (cm)] >  7.0\n",
      "4  |--------  class: 2\n",
      "1  |--  upper pivot: x[sepal width (cm)] >  3.2\n",
      "2  |----  lower pivot: x[petal width (cm)] <= 1.3\n",
      "3  |------  class: 0\n",
      "2  |----  upper pivot: x[petal width (cm)] >  1.3\n",
      "3  |------  lower pivot: x[sepal length (cm)] <= 6.475\n",
      "4  |--------  lower pivot: x[petal length (cm)] <= 5.25\n",
      "5  |----------  class: 1\n",
      "4  |--------  upper pivot: x[petal length (cm)] >  5.25\n",
      "5  |----------  class: 2\n",
      "3  |------  upper pivot: x[sepal length (cm)] >  6.475\n",
      "4  |--------  class: 2\n",
      "Predictions\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 2 1 2 2 1 1 2 1 1 1 1 2 1 1 2 1 2 1 2 1 2 2\n",
      " 1 1 2 2 2 1 1 1 1 2 2 1 2 1 1 1 1 2 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n",
      "Actual\n",
      "0      0\n",
      "1      0\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "      ..\n",
      "145    2\n",
      "146    2\n",
      "147    2\n",
      "148    2\n",
      "149    2\n",
      "Name: target, Length: 150, dtype: int64\n",
      "Accuracy: 0.8733333333333333\n",
      "Predictions\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n",
      "Actual\n",
      "0      0\n",
      "1      0\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "      ..\n",
      "145    2\n",
      "146    2\n",
      "147    2\n",
      "148    2\n",
      "149    2\n",
      "Name: target, Length: 150, dtype: int64\n",
      "Accuracy: 1.0\n",
      "['Index: [50 51 52 54 55 58 63 66 68 70 72 73 76 77 78 83 84 86 91], Ours: [2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2], Theirs: [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]']\n"
     ]
    }
   ],
   "source": [
    "def debug_tree(node, depth: int = 0, size: int = 1):\n",
    "    padding = '|' + ('--' * depth) + ' '\n",
    "\n",
    "    def p(o):\n",
    "        print(f'{str(depth).ljust(2)} {padding} {o}')\n",
    "\n",
    "    if isinstance(node, TerminalNode):\n",
    "        p(f'class: {node.value}')\n",
    "    elif isinstance(node, BranchNode):\n",
    "        p(f'lower pivot: {node.pivot.__str__(condition=True)}')\n",
    "        debug_tree(node.lower, depth=depth+size, size=size)\n",
    "        p(f'upper pivot: {node.pivot.__str__(condition=False)}')\n",
    "        debug_tree(node.upper, depth=depth+size, size=size)\n",
    "    elif isinstance(node, LookupNode):\n",
    "        for (k, v) in node.mapping.items():\n",
    "            p(f'lookup: {node.feature} {k}')\n",
    "            debug_tree(v, depth=depth+size, size=size)\n",
    "    else:\n",
    "        raise ValueError(f'Unexpected node: {node}')\n",
    "\n",
    "\n",
    "model: DecisionTree = DecisionTree()\n",
    "[xxx, yyy] = dataset(return_X_y=True, as_frame=True)\n",
    "\n",
    "# ddd = {\n",
    "#     'wind_direction': ['N', 'S', 'E', 'W'],\n",
    "#     'tide': ['Low', 'High'],\n",
    "#     'swell_forecasting': ['small', 'medium', 'large'],\n",
    "#     'good_waves': ['Yes', 'No'],\n",
    "# }\n",
    "#\n",
    "# # create an empty dataframe\n",
    "# df = pd.DataFrame(columns=ddd.keys())\n",
    "#\n",
    "# np.random.seed(42)\n",
    "# for i in range(len(xxx)):\n",
    "#     df.loc[i, 'wind_direction'] = str(np.random.choice(ddd['wind_direction'], 1)[0])\n",
    "#     df.loc[i, 'tide'] = str(np.random.choice(ddd['tide'], 1)[0])\n",
    "#     df.loc[i, 'swell_forecasting'] = str(np.random.choice(ddd['swell_forecasting'], 1)[0])\n",
    "#     df.loc[i, 'good_waves'] = str(np.random.choice(ddd['good_waves'], 1)[0])\n",
    "#     df.loc[i, 'temp'] = int(np.random.random() * 26) + 1\n",
    "#     df.loc[i, 'hello'] = 'world'\n",
    "#     for attr in xxx:\n",
    "#         df.loc[i, attr] = xxx.iloc[i][attr]\n",
    "#\n",
    "# # xxx = df.drop('good_waves', 1)\n",
    "# xxx = df\n",
    "# # yyy = df['good_waves']\n",
    "\n",
    "print(f'Data\\n{xxx}\\n')\n",
    "print(f'Targets\\n{yyy}\\n')\n",
    "\n",
    "model.fit(xxx, yyy)\n",
    "\n",
    "print(f'MAX DEPTH: {model.depth()}')\n",
    "debug_tree(model.root, 1, 1)\n",
    "\n",
    "predictions = model.predict(xxx)\n",
    "\n",
    "accuracy = accuracy_score(yyy, predictions)\n",
    "\n",
    "print(f'Predictions\\n{predictions}')\n",
    "print(f'Actual\\n{yyy}')\n",
    "print(f'Accuracy: {accuracy}')\n",
    "\n",
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(xxx, yyy)\n",
    "p = dt.predict(xxx)\n",
    "\n",
    "acc = accuracy_score(yyy, p)\n",
    "\n",
    "print(f'Predictions\\n{p}')\n",
    "print(f'Actual\\n{yyy}')\n",
    "print(f'Accuracy: {acc}')\n",
    "\n",
    "# differences = predictions.difference(p)\n",
    "# differences = np.nonzero(predictions - p)\n",
    "\n",
    "# differences = [f'Index: {idx}, Ours: {predictions[idx]}, Sklearns: {p[idx]}' for idx in index for index in np.nonzero(predictions - p)]\n",
    "diff = np.asarray(np.nonzero(predictions - p))\n",
    "ours = predictions[diff]\n",
    "theirs = p[diff]\n",
    "\n",
    "differences = [f'Index: {index}, Ours: {us}, Theirs: {them}' for (index, us, them) in zip(diff, ours, theirs)]\n",
    "\n",
    "print(differences)\n",
    "\n",
    "# TODO: python 3.6 compatibility (for google colab)\n",
    "# TODO: test_train_split / sampling\n",
    "# TODO: CRISP-DM\n",
    "# TODO: cleanup\n",
    "# TODO: documentation\n",
    "# TODO: dummy dataset (demonstrating edge cases) and then proper dataset (demonstrating capabilities)\n",
    "\n",
    "# interface Predicate:\n",
    "#     apply(element: any) -> bool;\n",
    "#\n",
    "# interface Node:\n",
    "#     eval(element: any) -> any;\n",
    "#\n",
    "# class BranchNode implements Node:\n",
    "#     condition: Predicate;\n",
    "#     left, right: Node;\n",
    "#\n",
    "#     eval(element: any) -> any:\n",
    "#         if condition.apply(element):\n",
    "#             return left.eval(element);\n",
    "#         else:\n",
    "#             return right.eval(element);\n",
    "#\n",
    "# class LeafNode implements Node:\n",
    "#     value: any;\n",
    "#\n",
    "#     eval(element: any) -> any:\n",
    "#         return value;"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}