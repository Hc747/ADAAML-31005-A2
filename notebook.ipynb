{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Advanced Data Analytics - Algorithms and Machine Learning\n",
    "## 31005\n",
    "### Harrison Cole\n",
    "### 12962712"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Section 1 - Imports\n",
    "Imports libraries and type-definitions for use throughout the program."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1329,
   "outputs": [],
   "source": [
    "import abc\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import traceback\n",
    "\n",
    "from typing import Callable, Optional, Union, Tuple, List, Dict\n",
    "from sklearn.datasets import load_iris, load_wine\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier, export_text\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Section 2 - Utility Function and Type Definitions\n",
    "Defines utility functions and types for (re)use throughout the program."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1330,
   "outputs": [],
   "source": [
    "def require(value: Optional[any], field: str) -> any:\n",
    "    \"\"\"\n",
    "    A mechanism for asserting the presence of a value, and raising an exception\n",
    "    in the case of its absence.\n",
    "    :param value:\n",
    "    The value whose presence is being checked.\n",
    "    :param field:\n",
    "    A diagnostic tag indicating which value is absent.\n",
    "    \"\"\"\n",
    "    if value is None:\n",
    "        raise ValueError(f'Missing required value: \"{field}\".')\n",
    "    return value\n",
    "\n",
    "\n",
    "def default(value: Optional[any], otherwise: any) -> any:\n",
    "    \"\"\"\n",
    "    A mechanism for checking for the presence of a value, and supplying a default value\n",
    "    in the case of its absence.\n",
    "    :param value:\n",
    "    The value whose presence is being checked.\n",
    "    :param otherwise:\n",
    "    The default value to return in the case of it's absence.\n",
    "    \"\"\"\n",
    "    return otherwise if value is None else value\n",
    "\n",
    "\n",
    "def value_counts(elements, normalise: bool = True) -> tuple:\n",
    "    \"\"\"\n",
    "    A mechanism for counting the occurrences of each unique value in a set of elements.\n",
    "    :param elements:\n",
    "    The set of elements.\n",
    "    :param normalise:\n",
    "    Whether or not to return the relative frequencies of the unique values.\n",
    "    :return:\n",
    "    The values and their corresponding representation within the set of elements\n",
    "    as a tuple of arrays.\n",
    "    \"\"\"\n",
    "    values, counts = np.unique(elements, return_counts=True)\n",
    "    if normalise:\n",
    "        return values, counts / np.sum(counts)\n",
    "    return values, counts\n",
    "\n",
    "\n",
    "# Convenience types describing common union types used throughout the application.\n",
    "FeatureType = Union[str, int]\n",
    "NumericType = Union[int, float]\n",
    "PredicateType = Callable[[any], bool]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Section 3 - Data-structures, Interfaces and Implementations\n",
    "Defines the API and data-structures available for use throughout this program. Where applicable, effort is taken\n",
    "to program by contract against the interface rather than the implementation."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Section 3.1 - Split Criterion Metrics"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1331,
   "outputs": [],
   "source": [
    "class SplitCriterionMetric(metaclass=abc.ABCMeta):\n",
    "    \"\"\"\n",
    "    An interface for computing the measure of quality produced by splitting the set of elements across\n",
    "    the axis of a given variable at each step of computation during the tree building process.\n",
    "    \"\"\"\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def compute(self, frequencies) -> float:\n",
    "        \"\"\"\n",
    "        Computes a measure of quality, usually the homogeneity (\"sameness\") of the target class, represented by the\n",
    "        frequencies of each target class instance within a subset of the dataset.\n",
    "        :param frequencies:\n",
    "        The frequencies of each target class instance within this subset.\n",
    "        :return:\n",
    "        A floating point value where higher values indicate a higher degree of homogeneity.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class Entropy(SplitCriterionMetric):\n",
    "\n",
    "    def compute(self, frequencies) -> float:\n",
    "        \"\"\"\n",
    "        Computes the entropy of the target class within a subset of the dataset.\n",
    "        :param frequencies:\n",
    "        The frequencies of each class instance within this subset.\n",
    "        :return:\n",
    "        A measure of the randomness of the distribution of each target class instance within this subset.\n",
    "        \"\"\"\n",
    "        eps=1e-9\n",
    "        return -(frequencies * np.log2(frequencies + eps)).sum()\n",
    "\n",
    "\n",
    "class GiniImpurity(SplitCriterionMetric):\n",
    "\n",
    "    def compute(self, frequencies) -> float:\n",
    "        \"\"\"\n",
    "        Computes the Gini impurity of the target class within a subset of the dataset.\n",
    "        :param frequencies:\n",
    "        The frequencies of each class instance within this subset.\n",
    "        :return:\n",
    "        A measure of how often a randomly chosen element from the dataset would be incorrectly labelled if\n",
    "        it was labelled according to the distribution of class instances within this subset.\n",
    "        \"\"\"\n",
    "        return 1 - np.sum(np.square(frequencies))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Section 3.2 - Pivot"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1332,
   "outputs": [],
   "source": [
    "class Pivot:\n",
    "    \"\"\"\n",
    "    A component class that captures and describes an arbitrary predicate that is\n",
    "    used as a pivot point for splitting a set of elements.\n",
    "    i.e.\n",
    "    elements = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "    predicate = lambda e: e <= 5\n",
    "    pivot = Pivot(predicate, (, ))\n",
    "    splits = [pivot.split(e) for e in elements]\n",
    "    [T, T, T, T, T, T, F, F, F, F, F]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, predicate: PredicateType, info: Tuple[any, any, str, str]):\n",
    "        self.__predicate = require(predicate, 'predicate')\n",
    "        self.__info = require(info, 'info')\n",
    "\n",
    "    @property\n",
    "    def predicate(self) -> PredicateType:\n",
    "        \"\"\"\n",
    "        A property returning the predicate captured in this pivot.\n",
    "        \"\"\"\n",
    "        return self.__predicate\n",
    "\n",
    "    def attribute(self) -> any:\n",
    "        \"\"\"\n",
    "        The value of the variable being pivoted upon.\n",
    "        \"\"\"\n",
    "        return self.__info[0]\n",
    "\n",
    "    def point(self) -> any:\n",
    "        \"\"\"\n",
    "        The value(s) of the pivot point.\n",
    "        \"\"\"\n",
    "        return self.__info[1]\n",
    "\n",
    "    def true_condition(self) -> str:\n",
    "        \"\"\"\n",
    "        The affirmative textual representation of the predicate.\n",
    "        \"\"\"\n",
    "        return self.__info[2]\n",
    "\n",
    "    def false_condition(self) -> str:\n",
    "        \"\"\"\n",
    "        The negative textual representation of the predicate.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return self.__info[3]\n",
    "\n",
    "    def split(self, value: any) -> bool:\n",
    "        \"\"\"\n",
    "        A mechanism for applying the predicate upon an element.\n",
    "        :param value:\n",
    "        The value upon which the predicate is applied.\n",
    "        \"\"\"\n",
    "        return self.predicate(value)\n",
    "\n",
    "    @staticmethod\n",
    "    def continuous(attribute: FeatureType, point: NumericType) -> 'Pivot':\n",
    "        \"\"\"\n",
    "        A static factory method for building a pivot that operates upon continuous (numerical)\n",
    "        values.\n",
    "        :param attribute:\n",
    "        The name (str) or index (int) that represents the key of the attribute value\n",
    "        upon which the pivot is applied within each element of a set of homogenous elements.\n",
    "        :param point:\n",
    "        The discrete value that represents the pivot point.\n",
    "        :return:\n",
    "        A pivot in the form of: lambda value: value[attribute] <= point\n",
    "        \"\"\"\n",
    "        def predicate(value: any) -> bool:\n",
    "            return value[attribute] <= point\n",
    "        return Pivot(predicate=predicate, info=(attribute, point, '<=', '>'))\n",
    "\n",
    "    def __str__(self, condition: bool = True) -> str:\n",
    "        \"\"\"\n",
    "        The human-intelligible, textual representation of this pivot.\n",
    "        :param condition:\n",
    "        The negation of the predicate, if false.\n",
    "        \"\"\"\n",
    "        operator: str = self.true_condition() if condition else self.false_condition()\n",
    "        return f'x[{self.attribute()}] {operator.ljust(2)} {self.point()}'\n",
    "\n",
    "\n",
    "class NumericalPivotCandidate:\n",
    "    \"\"\"\n",
    "    A component data-structure for tracking the set of parameters that best splits a\n",
    "    continuous attribute.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, feature: FeatureType, gain: float, probe: float):\n",
    "        self.__feature = feature\n",
    "        self.__gain = gain\n",
    "        self.__probe = probe\n",
    "\n",
    "    def feature(self) -> FeatureType:\n",
    "        \"\"\"\n",
    "        The name (str) or index (int) that represents the key of the attribute that is being\n",
    "        used as a feature.\n",
    "        \"\"\"\n",
    "        return require(self.__feature, 'feature')\n",
    "\n",
    "    def gain(self) -> float:\n",
    "        \"\"\"\n",
    "        The gain yielded by this combination of feature and probe value.\n",
    "        \"\"\"\n",
    "        return require(self.__gain, 'gain')\n",
    "\n",
    "    def probe(self) -> float:\n",
    "        \"\"\"\n",
    "        The best probe value tested thus far.\n",
    "        \"\"\"\n",
    "        return require(self.__probe, 'probe')\n",
    "\n",
    "    def update(self, feature: FeatureType, gain: float, probe: float) -> bool:\n",
    "        \"\"\"\n",
    "        Compares a new parameter set with the previously best seen parameter set and\n",
    "        updates the internal state of this data-structure if the new parameter set\n",
    "        yields a better gain.\n",
    "        :param feature:\n",
    "        The feature being evaluated.\n",
    "        :param gain:\n",
    "        The gain produced by this split.\n",
    "        :param probe:\n",
    "        The probe value used in producing this split.\n",
    "        :return:\n",
    "        True if this feature combination yielded a better gain than that yielded\n",
    "        by a previous combination, otherwise, False.\n",
    "        \"\"\"\n",
    "        if gain < self.gain():\n",
    "            return False\n",
    "        self.__feature = feature\n",
    "        self.__gain = gain\n",
    "        self.__probe = probe\n",
    "        return True\n",
    "\n",
    "    def is_valid(self) -> bool:\n",
    "        \"\"\"\n",
    "        Determines whether or not this candidate is suitable for use.\n",
    "        :return:\n",
    "        True if the gain is greater than -math.inf, false otherwise.\n",
    "        \"\"\"\n",
    "        return self.gain() > -math.inf\n",
    "\n",
    "    @staticmethod\n",
    "    def initial() -> 'NumericalPivotCandidate':\n",
    "        \"\"\"\n",
    "        A static factory method for initialising a default NumericalPivotCandidate data-structure\n",
    "        that has not seen previous parameter combinations.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return NumericalPivotCandidate(feature=0, gain=-math.inf, probe=1.0)\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\"\n",
    "        The human-intelligible, textual representation of the optimal parameter set.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return f'feature: {self.__feature}, gain: {self.__gain}, probe: {self.__probe}'\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Section 3.3 - Node"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1333,
   "outputs": [],
   "source": [
    "class Node(metaclass=abc.ABCMeta):\n",
    "    \"\"\"\n",
    "    An interface that encapsulates the logic of determining the class membership of an element\n",
    "    within the hierarchy of a decision tree.\n",
    "    \"\"\"\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def eval(self, element: any) -> any:\n",
    "        \"\"\"\n",
    "        Computes the class membership of an element, typically by means of traversing the node hierarchy\n",
    "        recursively.\n",
    "        :param element:\n",
    "        The element whose class we wish to determine.\n",
    "        :return:\n",
    "        The class membership of the given element.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError('Node#eval')\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def depth(self) -> int:\n",
    "        \"\"\"\n",
    "        Computes the number of levels (children) beneath this specific node, inclusive\n",
    "        of the current node.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def prune(self) -> 'Node':\n",
    "        \"\"\"\n",
    "        A mechanism for producing locally optimal or more efficient node configurations\n",
    "        dependent upon a nodes internal state. This method provides no default implementation\n",
    "        as optimisation is not necessary, although desirable, to the proper functioning of\n",
    "        a decision tree.\n",
    "        \"\"\"\n",
    "        return self\n",
    "\n",
    "    @staticmethod\n",
    "    def terminate(value: any) -> 'Node':\n",
    "        \"\"\"\n",
    "        A static factory method for producing a terminal (leaf) node with the given value.\n",
    "        :param value:\n",
    "        The class membership of this terminal node.\n",
    "        \"\"\"\n",
    "        return TerminalNode(value=value)\n",
    "\n",
    "    @staticmethod\n",
    "    def branch(pivot: 'Pivot', true_branch: Optional['Node'] = None, false_branch: Optional['Node'] = None) -> 'Node':\n",
    "        \"\"\"\n",
    "        A static factory method for producing a branch (internal) node with the given pivot and true/false branch nodes.\n",
    "        Used to handle mappings with a single split.\n",
    "        :param pivot:\n",
    "        The predicate upon which elements are pivoted.\n",
    "        :param true_branch:\n",
    "        The branch elements are directed to when the predicate evaluates true.\n",
    "        :param false_branch:\n",
    "        The branch elements are directed to when the predicate evaluates false.\n",
    "        \"\"\"\n",
    "        return BranchNode(pivot=pivot, true_branch=true_branch, false_branch=false_branch)\n",
    "\n",
    "    @staticmethod\n",
    "    def lookup(mapping: Dict[any, 'Node'], feature: FeatureType, default: any) -> 'Node':\n",
    "        \"\"\"\n",
    "        A static factory method for producing a branch (internal) node with the given attribute mappings.\n",
    "        Used to handle mappings with an arbitrary number of splits.\n",
    "        :param mapping:\n",
    "        The dictionary of arbitrary key to node mappings.\n",
    "        :param feature:\n",
    "        The feature whose value is applied as the lookup key against the given mapping.\n",
    "        \"\"\"\n",
    "        return LookupNode(mapping=mapping, feature=feature, default=default)\n",
    "\n",
    "\n",
    "class TerminalNode(Node):\n",
    "    \"\"\"\n",
    "    An implementation of the Node interface that statically resolves the class membership of an element.\n",
    "    Analogous to a leaf node in standard decision tree implementations.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, value: any):\n",
    "        self.__value = require(value, 'value')\n",
    "\n",
    "    def eval(self, element: any) -> any:\n",
    "        \"\"\"\n",
    "        Computes the class membership of the given element by statically mapping it to this node's value.\n",
    "        \"\"\"\n",
    "        return self.value\n",
    "\n",
    "    def depth(self) -> int:\n",
    "        \"\"\"\n",
    "        This implementation has no children, therefore its depth is always 1.\n",
    "        \"\"\"\n",
    "        return 1\n",
    "\n",
    "    @property\n",
    "    def value(self) -> any:\n",
    "        \"\"\"\n",
    "        The class membership value that this node evaluates to.\n",
    "        \"\"\"\n",
    "        return self.__value\n",
    "\n",
    "\n",
    "class BranchNode(Node):\n",
    "    \"\"\"\n",
    "    An implementation of the Node interface that dynamically resolves the class membership of an element.\n",
    "    Analogous to an internal node in standard decision tree implementations.\n",
    "    Used to handle mappings with a single split.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, pivot: 'Pivot', true_branch: 'Node', false_branch: 'Node'):\n",
    "        self.__pivot = require(pivot, 'pivot')\n",
    "        self.__nodes = np.asarray([false_branch, true_branch])\n",
    "\n",
    "    def eval(self, element: any) -> any:\n",
    "        \"\"\"\n",
    "        Dynamically computes the class membership of the given element by mapping it to another branch, by means of\n",
    "        applying the pivot's predicate, and then recursing until arriving at a terminal node.\n",
    "        \"\"\"\n",
    "        # Optimisation: removed the conditional branch to eliminate the cost incurred by miss-predicting a branch.\n",
    "        # previous form: branch: Node = self.true_branch if self.pivot.split(element) else self.false_branch\n",
    "        index: int = int(self.pivot.split(element))\n",
    "        branch: Node = self.nodes[index]\n",
    "        return branch.eval(element)\n",
    "\n",
    "    def depth(self) -> int:\n",
    "        \"\"\"\n",
    "        Computes the depth of this node and its deepest child.\n",
    "        \"\"\"\n",
    "        levels = [child.depth() for child in self.nodes]\n",
    "        return 1 + np.amax(levels, initial=0)\n",
    "\n",
    "    def prune(self) -> 'Node':\n",
    "        \"\"\"\n",
    "        This mechanism attempts to optimise the internal structure of this node.\n",
    "        - eliminates the branch condition if possible.\n",
    "        \"\"\"\n",
    "        if isinstance(self.true_branch, TerminalNode) and isinstance(self.false_branch, TerminalNode):\n",
    "            if self.true_branch.value == self.false_branch.value:\n",
    "                return self.true_branch\n",
    "        return self\n",
    "\n",
    "    @property\n",
    "    def pivot(self) -> 'Pivot':\n",
    "        \"\"\"\n",
    "        The pivot condition of this node.\n",
    "        \"\"\"\n",
    "        return self.__pivot\n",
    "\n",
    "    @property\n",
    "    def nodes(self):\n",
    "        \"\"\"\n",
    "        The array of nodes representing each branch.\n",
    "        Always of length 2 in the form [false_branch, true_branch]\n",
    "        \"\"\"\n",
    "        return self.__nodes\n",
    "\n",
    "    @property\n",
    "    def true_branch(self) -> 'Node':\n",
    "        \"\"\"\n",
    "        The node applied when the pivot condition evaluates truthfully.\n",
    "        \"\"\"\n",
    "        return self.nodes[int(True)]\n",
    "\n",
    "    @property\n",
    "    def false_branch(self) -> 'Node':\n",
    "        \"\"\"\n",
    "        The node applied when the pivot condition evaluates falsely.\n",
    "        \"\"\"\n",
    "        return self.nodes[int(False)]\n",
    "\n",
    "\n",
    "class LookupNode(Node):\n",
    "    \"\"\"\n",
    "    An implementation of the Node interface that dynamically resolves the class membership of an element.\n",
    "    An extended version of the internal node in standard decision tree implementations that is used to handle\n",
    "    mappings with an arbitrary number of splits.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mapping: Dict[any, 'Node'], feature: FeatureType, default: any):\n",
    "        self.__mapping = require(mapping, 'mapping')\n",
    "        self.__feature = require(feature, 'feature')\n",
    "        self.__default = require(default, 'default')\n",
    "\n",
    "    def eval(self, element: any) -> any:\n",
    "        \"\"\"\n",
    "        Dynamically computes the class membership of the given element by mapping it to another branch, by means of\n",
    "        looking up the feature value within the node's mapping, and then recursing until arriving at a terminal node.\n",
    "        \"\"\"\n",
    "        key = element[self.feature]\n",
    "        lookup: Node = self.mapping.get(key, None)\n",
    "        if lookup is None:\n",
    "            return self.default\n",
    "        return lookup.eval(element)\n",
    "\n",
    "    def depth(self, level: int = 0) -> int:\n",
    "        \"\"\"\n",
    "        Computes the depth of this node and its deepest child.\n",
    "        \"\"\"\n",
    "        levels = [child.depth() for child in self.mapping.values()]\n",
    "        return 1 + np.amax(levels, initial=0)\n",
    "\n",
    "    def prune(self) -> 'Node':\n",
    "        \"\"\"\n",
    "        This mechanism attempts to optimise the internal structure of this node.\n",
    "        - eliminates the mapping if there is only one element.\n",
    "        N.B. This type of node could be subject to much further optimisation efforts.\n",
    "        \"\"\"\n",
    "        size = len(self.mapping)\n",
    "        if size == 1:\n",
    "            return next(iter(self.mapping.values()))\n",
    "        return self\n",
    "\n",
    "    @property\n",
    "    def mapping(self) -> Dict[any, 'Node']:\n",
    "        \"\"\"\n",
    "        The internal mapping between feature values and nodes.\n",
    "        \"\"\"\n",
    "        return self.__mapping\n",
    "\n",
    "    @property\n",
    "    def feature(self) -> FeatureType:\n",
    "        \"\"\"\n",
    "        The feature whose values are mapped by this node.\n",
    "        \"\"\"\n",
    "        return self.__feature\n",
    "\n",
    "    @property\n",
    "    def default(self) -> any:\n",
    "        \"\"\"\n",
    "        The default value to return if the feature mapping fails.\n",
    "        \"\"\"\n",
    "        return self.__default\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Section 3.4 - Decision Tree Builders"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1334,
   "outputs": [],
   "source": [
    "# A mapping of all available implementations of the SplitCriterionMetric interface.\n",
    "metrics: Dict[str, SplitCriterionMetric] = {\n",
    "    'entropy': Entropy(),\n",
    "    'gini': GiniImpurity()\n",
    "}\n",
    "\n",
    "\n",
    "# The default implementation of the SplitCriterionMetric to use in tree construction.\n",
    "DEFAULT_CRITERION: str = 'entropy'\n",
    "\n",
    "\n",
    "def get_metric(name: str) -> SplitCriterionMetric:\n",
    "    \"\"\"\n",
    "    A utility function for safely retrieving a split criterion metric of the given name.\n",
    "    \"\"\"\n",
    "    key: str = name if name in metrics.keys() else DEFAULT_CRITERION\n",
    "    return require(metrics.get(key), f'metric => {name}')\n",
    "\n",
    "\n",
    "class DecisionTreeBuilder(metaclass=abc.ABCMeta):\n",
    "    \"\"\"\n",
    "    An interface that encapsulates the logic of constructing the internal node hierarchy of decision trees.\n",
    "    \"\"\"\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def build(self, x, y, **kwargs) -> 'Node':\n",
    "        \"\"\"\n",
    "        The public method responsible for constructing the node hierarchy according to some node construction\n",
    "        algorithm, i.e., CART, ID3, C4.5, etc.\n",
    "        :param x:\n",
    "        The set of data elements.\n",
    "        :param y:\n",
    "        The set of class membership labels corresponding to each element in the set of data elements.\n",
    "        :param kwargs:\n",
    "        Any additional parameters passed to the node construction algorithm.\n",
    "        :return:\n",
    "        The root node of a decision tree representing the node hierarchy.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError('DecisionTreeBuilder#build')\n",
    "\n",
    "    def compute_metric(self, attributes, criterion: str) -> float:\n",
    "        \"\"\"\n",
    "        A mechanism for computing the split value for the given collection of attributes.\n",
    "        :param attributes:\n",
    "        The set of attribute values.\n",
    "        :param criterion:\n",
    "        The name of the split criterion metric to use.\n",
    "        :return:\n",
    "        The floating point representation of the metric that was computed upon the relative frequencies\n",
    "        of each attribute's value within the set of values.\n",
    "        \"\"\"\n",
    "        _, frequencies = value_counts(attributes, normalise=True)\n",
    "        metric: SplitCriterionMetric = get_metric(criterion)\n",
    "        return metric.compute(frequencies=frequencies)\n",
    "\n",
    "    # TODO: optimise\n",
    "    def information_gain_categorical(self, target, feature, criterion: str) -> float:\n",
    "        \"\"\"\n",
    "        A mechanism for computing the split value for the given collection of categorical attributes.\n",
    "        :param target:\n",
    "        The set of target values.\n",
    "        :param feature:\n",
    "        The set of feature values.\n",
    "        :param criterion:\n",
    "        The name of the split criterion metric to use.\n",
    "        :return:\n",
    "        The information gain yielded by performing the split against this feature.\n",
    "        \"\"\"\n",
    "        total_info = self.compute_metric(target, criterion=criterion)\n",
    "        split_info = self.compute_metric(feature, criterion=criterion)\n",
    "        result: float = total_info - split_info\n",
    "        return result\n",
    "\n",
    "    # TODO: optimise\n",
    "    def information_gain_continuous(self, target, feature, point, criterion: str) -> float:\n",
    "        \"\"\"\n",
    "        A mechanism for computing the split value for the given collection of continuous attributes split at the\n",
    "        given point value.\n",
    "        :param target:\n",
    "        The set of target values.\n",
    "        :param feature:\n",
    "        The set of feature values.\n",
    "        :param point:\n",
    "        The pivot point value.\n",
    "        :param criterion:\n",
    "        The name of the split criterion metric to use.\n",
    "        :return:\n",
    "        The information gain yielded by performing the split against this feature at the given pivot point.\n",
    "        \"\"\"\n",
    "        size, split_indices = len(target), np.asarray([feature <= point, feature > point])\n",
    "        total_info = self.compute_metric(target, criterion=criterion)\n",
    "        split_info = np.sum(np.asarray([(self.compute_metric(feature[index], criterion=criterion) * (np.count_nonzero(index) / size)) for index in split_indices]))\n",
    "        result: float = total_info - split_info\n",
    "        return result\n",
    "\n",
    "    @staticmethod\n",
    "    def factory(implementation: str, **kwargs) -> 'DecisionTreeBuilder':\n",
    "        \"\"\"\n",
    "        A static factory method for constructing an instance of DecisionTreeBuilder.\n",
    "        :param implementation:\n",
    "        The name of the implementation to use, i.e., ID3.\n",
    "        :param kwargs:\n",
    "        A map of arguments to supply the constructor of the DecisionTreeBuilder implementation.\n",
    "        :return:\n",
    "        A constructed instance of DecisionTreeBuilder.\n",
    "        \"\"\"\n",
    "        factories = {\n",
    "            'ID3': ID3DecisionTreeBuilder\n",
    "        }\n",
    "        constructor = require(factories.get(implementation, None), implementation)\n",
    "        return constructor(**kwargs)\n",
    "\n",
    "    @staticmethod\n",
    "    def default() -> 'DecisionTreeBuilder':\n",
    "        \"\"\"\n",
    "        Constructs a reasonably selected default implementation of the DecisionTreeBuilder interface.\n",
    "        Currently the default implementation is ID3.\n",
    "        \"\"\"\n",
    "        return DecisionTreeBuilder.factory('ID3')\n",
    "\n",
    "\n",
    "class ID3DecisionTreeBuilder(DecisionTreeBuilder):\n",
    "    \"\"\"\n",
    "    An implementation of the DecisionTreeBuilder interface that constructs internal node hierarchy of a decision tree\n",
    "    as per the specification of the Iterative Dichotomiser 3 (ID3) algorithm by Ross Quinlan.\n",
    "\n",
    "    The specification can be found at the following location: https://en.wikipedia.org/wiki/ID3_algorithm#Algorithm\n",
    "    \"\"\"\n",
    "\n",
    "    def build(self, x, y, criterion: str = DEFAULT_CRITERION) -> 'Node':\n",
    "        \"\"\"\n",
    "        The public API for constructing the ID3 node hierarchy for the given set of elements and class membership labels.\n",
    "        :param x:\n",
    "        The set of data elements.\n",
    "        :param y:\n",
    "        The set of class membership labels.\n",
    "        :param criterion:\n",
    "        The name of the split criterion metric to use.\n",
    "        :return:\n",
    "        The constructed node hierarchy.\n",
    "        \"\"\"\n",
    "        data = x.copy()\n",
    "        data[y.name] = y\n",
    "        return self._build(data=data, features=x.columns, target=y.name, criterion=criterion)\n",
    "\n",
    "    def _build(self, data, features, target: FeatureType, criterion: str, parent_class=None) -> 'Node':\n",
    "        \"\"\"\n",
    "        The internal API for constructing the ID3 node hierarchy.\n",
    "        :param data:\n",
    "        The set of elements and target values.\n",
    "        :param features:\n",
    "        The set of features.\n",
    "        :param target:\n",
    "        The target attribute to predict.\n",
    "        :param criterion:\n",
    "        The name of the split criterion metric to use.\n",
    "        :param parent_class:\n",
    "        The majority class of the previous node (if any).\n",
    "        :return:\n",
    "        The constructed ID3 node hierarchy.\n",
    "        \"\"\"\n",
    "        target_subset = data[target]\n",
    "        classes, counts = value_counts(target_subset, normalise=False)\n",
    "        # classes = np.unique(target_subset)\n",
    "        choices = len(classes)\n",
    "\n",
    "        # base case #1\n",
    "        # There are no examples in the subset, which happens when no example in the parent set was found to match a\n",
    "        # specific value of the selected attribute.\n",
    "        if len(data) <= 0:\n",
    "            return Node.terminate(parent_class)\n",
    "\n",
    "        # base case #2\n",
    "        # Every element of the subset belongs to the same class.\n",
    "        if choices <= 1:\n",
    "            return Node.terminate(classes[0])\n",
    "\n",
    "        majority_class = classes[np.argmax(counts)]\n",
    "\n",
    "        # base case #3\n",
    "        # There are no more attributes to be selected, but the examples still do not belong to the same class.\n",
    "        if len(features) <= 0:\n",
    "            return Node.terminate(majority_class)\n",
    "\n",
    "        # noinspection PyBroadException\n",
    "        try:\n",
    "            # attempt to build the subtree\n",
    "            subtree: Node = self._build_continuous(data=data, features=features, target=target, criterion=criterion, parent_class=majority_class)\n",
    "        except:\n",
    "            subtree: Node = self._build_categorical(data=data, features=features, target=target, criterion=criterion, parent_class=majority_class)\n",
    "\n",
    "        pruned: Node = subtree.prune()\n",
    "        while pruned != subtree:\n",
    "            subtree = pruned\n",
    "            pruned = subtree.prune()\n",
    "\n",
    "        # more elegantly expresses the above in python 3.8+\n",
    "        # while (pruned := subtree.prune()) != subtree:\n",
    "        #     subtree = pruned\n",
    "\n",
    "        return subtree\n",
    "\n",
    "    def _build_continuous(self, data, features, target, criterion: str, parent_class) -> 'Node':\n",
    "\n",
    "        def build_subtree(indices, remaining_features) -> 'Node':\n",
    "            return self._build(data=data[indices], features=remaining_features, target=target, criterion=criterion, parent_class=parent_class)\n",
    "\n",
    "        candidate: NumericalPivotCandidate = NumericalPivotCandidate.initial()\n",
    "        objective = data[target]\n",
    "\n",
    "        for feature in features:\n",
    "            attribute = data[feature]\n",
    "            try:\n",
    "                points = self.create_probe_values(attribute.min(), attribute.max())\n",
    "                for point in points:\n",
    "                    gain = self.information_gain_continuous(target=objective, feature=attribute, point=point, criterion=criterion)\n",
    "                    candidate.update(feature=feature, gain=gain, probe=point)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        if not candidate.is_valid():\n",
    "            raise ValueError(\"No valid pivot candidate found.\")\n",
    "\n",
    "        pivot: Pivot = Pivot.continuous(candidate.feature(), candidate.probe())\n",
    "        remaining = [feature for feature in features if feature != candidate.feature()]\n",
    "\n",
    "        true_branch = build_subtree(indices=data[candidate.feature()] <= candidate.probe(), remaining_features=remaining)\n",
    "        false_branch = build_subtree(indices=data[candidate.feature()] > candidate.probe(), remaining_features=remaining)\n",
    "\n",
    "        return Node.branch(pivot=pivot, true_branch=true_branch, false_branch=false_branch)\n",
    "\n",
    "    def _build_categorical(self, data, features, target, criterion: str, parent_class) -> 'Node':\n",
    "\n",
    "        def build_subtree(subset, remaining_features) -> 'Node':\n",
    "            return self._build(data=subset, features=remaining_features, target=target, criterion=criterion, parent_class=parent_class)\n",
    "\n",
    "        objective = data[target]\n",
    "        gains = np.asarray([self.information_gain_categorical(target=objective, feature=data[feature], criterion=criterion) for feature in features])\n",
    "        best_feature = features[np.argmax(gains)]\n",
    "        attribute = data[best_feature]\n",
    "        remaining = [feature for feature in features if feature != best_feature]\n",
    "\n",
    "        values, counts = value_counts(attribute, normalise=False)\n",
    "        default_value = values[np.argmax(counts)]\n",
    "        mapping: Dict[any, Node] = {}\n",
    "\n",
    "        for value in values:\n",
    "            mapping[value] = build_subtree(subset=data.where(attribute == value).dropna(), remaining_features=remaining)\n",
    "\n",
    "        return Node.lookup(mapping=mapping, feature=best_feature, default=default_value)\n",
    "\n",
    "    def create_probe_values(self, minima, maxima):\n",
    "        weights = np.arange(0.0, 1.0, 0.05)\n",
    "        return np.asarray([point * minima + (1.0 - point) * maxima for point in weights])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1335,
   "outputs": [],
   "source": [
    "class Model(metaclass=abc.ABCMeta):\n",
    "\n",
    "    def compile(self, **kwargs):\n",
    "        pass\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def fit(self, x, y, **kwargs):\n",
    "        raise NotImplementedError('Model#fit')\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def predict(self, x, **kwargs):\n",
    "        raise NotImplementedError('Model#predict')\n",
    "\n",
    "\n",
    "class DecisionTree(Model):\n",
    "\n",
    "    # TODO: default value (most common class..?)\n",
    "    __builder: 'DecisionTreeBuilder' = DecisionTreeBuilder.default()\n",
    "    __root: Optional['Node'] = None\n",
    "    __depth: Optional[int] = None\n",
    "\n",
    "    def compile(self, **kwargs):\n",
    "        previous: DecisionTreeBuilder = self.__builder\n",
    "        try:\n",
    "            builder = DecisionTreeBuilder.factory(kwargs['implementation'], **kwargs)\n",
    "        except (KeyError, ValueError):\n",
    "            builder = previous\n",
    "        self.__builder = builder\n",
    "\n",
    "    def fit(self, x, y, **kwargs):\n",
    "        self.__root = self.builder.build(x, y, **kwargs)\n",
    "        self.__depth = None\n",
    "\n",
    "    def predict(self, x, **kwargs):\n",
    "        tree: Node = self.root\n",
    "        samples = x.to_dict(orient='records')\n",
    "        return np.asarray([tree.eval(sample) for sample in samples])\n",
    "\n",
    "    @property\n",
    "    def builder(self) -> 'DecisionTreeBuilder':\n",
    "        return require(self.__builder, 'builder')\n",
    "\n",
    "    @property\n",
    "    def root(self) -> 'Node':\n",
    "        return require(self.__root, 'root')\n",
    "\n",
    "    def depth(self) -> int:\n",
    "        depth: int\n",
    "        if self.__root is None:\n",
    "            # the tree has not been built.\n",
    "            depth = 0\n",
    "        elif self.__depth is not None:\n",
    "            # the depth is cached as it has previously been calculated and the tree has not changed.\n",
    "            depth = self.__depth\n",
    "        else:\n",
    "            # the depth has not been calculated prior to this invocation.\n",
    "            depth = self.__depth = self.root.depth()\n",
    "        return depth"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Section 4 - Implementation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1336,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- RESULTS -- iris -- entropy ---\n",
      "Our Accuracy:   0.9111111111111111\n",
      "Their Accuracy: 1.0\n",
      "Differences: [ 3 32 35 42]\n",
      "Ours:        [2 2 1 2]\n",
      "Theirs:      [1 1 2 1]\n",
      "Correct:     [1 1 2 1]\n",
      "\n",
      "--- RESULTS -- iris -- gini ---\n",
      "Our Accuracy:   0.7333333333333333\n",
      "Their Accuracy: 1.0\n",
      "Differences: [ 0  1  3  4  5  7  8 10 21 25 31 43]\n",
      "Ours:        [2 1 2 2 1 1 2 1 1 1 1 1]\n",
      "Theirs:      [1 0 1 1 0 2 1 2 2 2 0 0]\n",
      "Correct:     [1 0 1 1 0 2 1 2 2 2 0 0]\n",
      "\n",
      "--- RESULTS -- wine -- entropy ---\n",
      "Our Accuracy:   0.7222222222222222\n",
      "Their Accuracy: 0.8148148148148148\n",
      "Differences: [ 1  2  6  9 12 13 23 29 33 36 44 45 46 48 50]\n",
      "Ours:        [1 2 0 2 1 1 1 1 1 0 0 1 0 1 1]\n",
      "Theirs:      [0 0 1 0 0 2 2 0 0 1 1 0 1 2 0]\n",
      "Correct:     [0 2 1 2 0 1 2 0 0 2 2 0 1 2 1]\n",
      "\n",
      "--- RESULTS -- wine -- gini ---\n",
      "Our Accuracy:   0.8148148148148148\n",
      "Their Accuracy: 0.9444444444444444\n",
      "Differences: [ 8 11 23 27 36 39 40 42 43 53]\n",
      "Ours:        [0 1 1 0 1 1 0 0 0 0]\n",
      "Theirs:      [1 0 2 1 2 2 1 1 1 2]\n",
      "Correct:     [1 2 2 1 2 2 0 1 1 2]\n",
      "\n",
      "--- RESULTS -- junk -- entropy ---\n",
      "Our Accuracy:   0.4\n",
      "Their Accuracy: Exception occurred during processing: could not convert string to float: 'W'\n",
      "Differences: N/A\n",
      "Ours:        N/A\n",
      "Theirs:      N/A\n",
      "Correct:     N/A\n",
      "\n",
      "--- RESULTS -- junk -- gini ---\n",
      "Our Accuracy:   0.28888888888888886\n",
      "Their Accuracy: Exception occurred during processing: could not convert string to float: 'W'\n",
      "Differences: N/A\n",
      "Ours:        N/A\n",
      "Theirs:      N/A\n",
      "Correct:     N/A\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def debug_tree(node, depth: int = 0):\n",
    "    if isinstance(node, DecisionTreeClassifier):\n",
    "        print(export_text(node))\n",
    "        return\n",
    "    if isinstance(node, DecisionTree):\n",
    "        debug_tree(node.root)\n",
    "        return\n",
    "\n",
    "    padding = ('|   ' * depth)[:-3] + '---'\n",
    "\n",
    "    def p(o):\n",
    "        print(f'{str(depth).ljust(2)} {padding} {o}')\n",
    "\n",
    "    if isinstance(node, TerminalNode):\n",
    "        p(f'class: {node.value}')\n",
    "    elif isinstance(node, BranchNode):\n",
    "        p(f'{node.pivot.__str__(condition=True)}')\n",
    "        debug_tree(node.true_branch, depth=depth+1,)\n",
    "        p(f'{node.pivot.__str__(condition=False)}')\n",
    "        debug_tree(node.false_branch, depth=depth+1)\n",
    "    elif isinstance(node, LookupNode):\n",
    "        for (k, v) in node.mapping.items():\n",
    "            p(f'lookup: {node.feature} {k}')\n",
    "            debug_tree(v, depth=depth+1)\n",
    "        p(f'default: {node.feature} {node.default}')\n",
    "    else:\n",
    "        raise ValueError(f'Unexpected node: {node}')\n",
    "\n",
    "\n",
    "def junk_mixed_dataset(samples: int = 100):\n",
    "    attribute_pool = {\n",
    "        'wind_direction': ['N', 'S', 'E', 'W'],\n",
    "        'tide': ['Low', 'High'],\n",
    "        'swell_forecasting': ['small', 'medium', 'large'],\n",
    "        'good_waves': ['Yes', 'No'],\n",
    "        'temp': '',\n",
    "        'hello': ''\n",
    "    }\n",
    "    df = pd.DataFrame(columns=attribute_pool.keys())\n",
    "    np.random.seed(42)\n",
    "    for i in range(samples):\n",
    "        df.loc[i, 'wind_direction'] = str(np.random.choice(attribute_pool['wind_direction'], 1)[0]) # categorical\n",
    "        df.loc[i, 'tide'] = str(np.random.choice(attribute_pool['tide'], 1)[0]) # categorical\n",
    "        df.loc[i, 'swell_forecasting'] = str(np.random.choice(attribute_pool['swell_forecasting'], 1)[0]) # categorical\n",
    "        df.loc[i, 'good_waves'] = str(np.random.choice(attribute_pool['good_waves'], 1)[0]) # categorical\n",
    "        df.loc[i, 'good_waves_int'] = int(np.random.choice([0, 1], 1)[0]) # numeric somewhat correlated\n",
    "        df.loc[i, 'temp'] = int(np.random.random() * 26) + 1 # numeric but irrelevant\n",
    "        df.loc[i, 'hello'] = 'world' # categorical but irrelevant\n",
    "\n",
    "    target_attr = df['good_waves'].copy()\n",
    "    df.drop(labels=['good_waves'], axis=1, inplace=True)\n",
    "    return df, target_attr\n",
    "\n",
    "datasets = {\n",
    "    'iris': lambda: load_iris(return_X_y=True, as_frame=True),\n",
    "    'wine': lambda: load_wine(return_X_y=True, as_frame=True),\n",
    "    'junk': lambda: junk_mixed_dataset(150)\n",
    "}\n",
    "\n",
    "for (name, provider) in datasets.items():\n",
    "    [elements, labels] = provider()\n",
    "    random_state: int = 42\n",
    "    train_split_size: float = 0.7\n",
    "    debug: bool = False\n",
    "\n",
    "    for metric in ['entropy', 'gini']:\n",
    "\n",
    "        if debug:\n",
    "            print(f'--- DATASET -- {name} -- {metric} ---')\n",
    "            print(f'Elements:\\n\\n{elements[:5]}\\n')\n",
    "            print(f'Targets:\\n\\n{labels[:5]}\\n')\n",
    "\n",
    "        train_x, test_x = train_test_split(elements, train_size=train_split_size, random_state=random_state)\n",
    "        train_y, test_y = train_test_split(labels, train_size=train_split_size, random_state=random_state)\n",
    "\n",
    "        ours: DecisionTree = DecisionTree()\n",
    "        theirs: DecisionTreeClassifier = DecisionTreeClassifier(criterion=metric)\n",
    "\n",
    "        def evaluate(text: str, classifier):\n",
    "            try:\n",
    "                if isinstance(classifier, DecisionTree):\n",
    "                    classifier.fit(train_x, train_y, criterion=metric)\n",
    "                else:\n",
    "                    classifier.fit(train_x, train_y)\n",
    "\n",
    "                if debug:\n",
    "                    print()\n",
    "                    print(f'--- {text} ---')\n",
    "                    debug_tree(classifier)\n",
    "                    print(f'--- {text} ---')\n",
    "                    print()\n",
    "\n",
    "                predictions = classifier.predict(test_x)\n",
    "                accuracy = accuracy_score(test_y, predictions)\n",
    "                return accuracy, predictions\n",
    "            except Exception as e:\n",
    "                return f'Exception occurred during processing: {e}', None\n",
    "\n",
    "        us = evaluate('ours', ours)\n",
    "        them = evaluate('theirs', theirs)\n",
    "\n",
    "        if them[1] is not None:\n",
    "            differences = np.nonzero(us[1] - them[1])\n",
    "        else:\n",
    "            differences = None\n",
    "\n",
    "        print(f'--- RESULTS -- {name} -- {metric} ---')\n",
    "        print(f'Our Accuracy:   {us[0]}')\n",
    "        print(f'Their Accuracy: {them[0]}')\n",
    "        print(f'Differences: {np.asarray(differences)[0] if them[1] is not None else \"N/A\"}')\n",
    "        print(f'Ours:        {us[1][differences] if them[1] is not None else \"N/A\"}')\n",
    "        print(f'Theirs:      {them[1][differences] if them[1] is not None else \"N/A\"}')\n",
    "        print(f'Correct:     {np.asarray(test_y)[differences] if them[1] is not None else \"N/A\"}')\n",
    "        print()\n",
    "\n",
    "\n",
    "# # TODO: python 3.6 compatibility (for google colab)\n",
    "# # TODO: CRISP-DM"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}