{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Advanced Data Analytics - Algorithms and Machine Learning\n",
    "## 31005\n",
    "### Harrison Cole\n",
    "### 12962712"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Section 1 - Imports\n",
    "Imports libraries and type-definitions for use throughout the program."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "import abc\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from typing import Callable, Optional, Union, Tuple, Dict\n",
    "from sklearn.datasets import load_iris, load_wine\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier, export_text\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Section 2 - Utility Function and Type Definitions\n",
    "Defines utility functions and types for (re)use throughout the program."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "def require(value: Optional[any], field: str) -> any:\n",
    "    \"\"\"\n",
    "    A mechanism for asserting the presence of a value, and raising an exception\n",
    "    in the case of its absence.\n",
    "    :param value:\n",
    "    The value whose presence is being checked.\n",
    "    :param field:\n",
    "    A diagnostic tag indicating which value is absent.\n",
    "    \"\"\"\n",
    "    if value is None:\n",
    "        raise ValueError(f'Missing required value: \"{field}\".')\n",
    "    return value\n",
    "\n",
    "\n",
    "def default(value: Optional[any], otherwise: any) -> any:\n",
    "    \"\"\"\n",
    "    A mechanism for checking for the presence of a value, and supplying a default value\n",
    "    in the case of its absence.\n",
    "    :param value:\n",
    "    The value whose presence is being checked.\n",
    "    :param otherwise:\n",
    "    The default value to return in the case of it's absence.\n",
    "    \"\"\"\n",
    "    return otherwise if value is None else value\n",
    "\n",
    "\n",
    "def value_counts(elements, normalise: bool = True) -> tuple:\n",
    "    \"\"\"\n",
    "    A mechanism for counting the occurrences of each unique value in a set of elements.\n",
    "    :param elements:\n",
    "    The set of elements.\n",
    "    :param normalise:\n",
    "    Whether or not to return the relative frequencies of the unique values.\n",
    "    :return:\n",
    "    The values and their corresponding representation within the set of elements\n",
    "    as a tuple of arrays.\n",
    "    \"\"\"\n",
    "    values, counts = np.unique(elements, return_counts=True)\n",
    "    if normalise:\n",
    "        return values, counts / np.sum(counts)\n",
    "    return values, counts\n",
    "\n",
    "\n",
    "def majority_class_index(elements):\n",
    "    \"\"\"\n",
    "    A mechanism for returning the index of the class with the greatest representation\n",
    "    in a set of elements.\n",
    "    :param elements:\n",
    "    The set of elements.\n",
    "    \"\"\"\n",
    "    _, counts = value_counts(elements, normalise=False)\n",
    "    return np.argmax(counts)\n",
    "\n",
    "\n",
    "FeatureType = Union[str, int]\n",
    "NumericType = Union[int, float]\n",
    "PredicateType = Callable[[any], bool]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Section 3 - Data-structures, Interfaces and Implementations\n",
    "Defines the API and data-structures available for use throughout this program. Where applicable, effort is taken\n",
    "to program by contract against the interface rather than the implementation."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Section 3.1 - Split Criterion Metrics"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "class SplitCriterionMetric(metaclass=abc.ABCMeta):\n",
    "    \"\"\"\n",
    "    An interface for computing the measure of quality produced by splitting the set of elements across\n",
    "    the axis of a given variable at each step of computation during the tree building process.\n",
    "    \"\"\"\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def compute(self, frequencies) -> float:\n",
    "        \"\"\"\n",
    "        Computes a measure of quality, usually the homogeneity (\"sameness\") of the target class, represented by the\n",
    "        frequencies of each target class instance within a subset of the dataset.\n",
    "        :param frequencies:\n",
    "        The frequencies of each target class instance within this subset.\n",
    "        :return:\n",
    "        A floating point value where higher values indicate a higher degree of homogeneity.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class Entropy(SplitCriterionMetric):\n",
    "\n",
    "    def compute(self, frequencies) -> float:\n",
    "        \"\"\"\n",
    "        Computes the entropy of the target class within a subset of the dataset.\n",
    "        :param frequencies:\n",
    "        The frequencies of each class instance within this subset.\n",
    "        :return:\n",
    "        A measure of the randomness of the distribution of each target class instance within this subset.\n",
    "        \"\"\"\n",
    "        eps=1e-9\n",
    "        return -(frequencies * np.log2(frequencies + eps)).sum()\n",
    "\n",
    "\n",
    "class GiniImpurity(SplitCriterionMetric):\n",
    "\n",
    "    def compute(self, frequencies) -> float:\n",
    "        \"\"\"\n",
    "        Computes the Gini impurity of the target class within a subset of the dataset.\n",
    "        :param frequencies:\n",
    "        The frequencies of each class instance within this subset.\n",
    "        :return:\n",
    "        A measure of how often a randomly chosen element from the dataset would be incorrectly labelled if\n",
    "        it was labelled according to the distribution of class instances within this subset.\n",
    "        \"\"\"\n",
    "        return 1 - np.sum(np.square(frequencies))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Section 3.2 - Pivot"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "class Pivot:\n",
    "    \"\"\"\n",
    "    A component class that captures and describes an arbitrary predicate that is\n",
    "    used as a pivot point for splitting a set of elements.\n",
    "    i.e.\n",
    "    elements = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "    predicate = lambda e: e <= 5\n",
    "    pivot = Pivot(predicate, (, ))\n",
    "    splits = [pivot.split(e) for e in elements]\n",
    "    [T, T, T, T, T, T, F, F, F, F, F]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, predicate: PredicateType, info: Tuple[any, any, str, str]):\n",
    "        self.__predicate = require(predicate, 'predicate')\n",
    "        self.__info = require(info, 'info')\n",
    "\n",
    "    @property\n",
    "    def predicate(self) -> PredicateType:\n",
    "        \"\"\"\n",
    "        A property returning the predicate captured in this pivot.\n",
    "        \"\"\"\n",
    "        return self.__predicate\n",
    "\n",
    "    def attribute(self) -> any:\n",
    "        \"\"\"\n",
    "        The value of the variable being pivoted upon.\n",
    "        \"\"\"\n",
    "        return self.__info[0]\n",
    "\n",
    "    def point(self) -> any:\n",
    "        \"\"\"\n",
    "        The value(s) of the pivot point.\n",
    "        \"\"\"\n",
    "        return self.__info[1]\n",
    "\n",
    "    def true_condition(self) -> str:\n",
    "        \"\"\"\n",
    "        The affirmative textual representation of the predicate.\n",
    "        \"\"\"\n",
    "        return self.__info[2]\n",
    "\n",
    "    def false_condition(self) -> str:\n",
    "        \"\"\"\n",
    "        The negative textual representation of the predicate.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return self.__info[3]\n",
    "\n",
    "    def split(self, value: any) -> bool:\n",
    "        \"\"\"\n",
    "        A mechanism for applying the predicate upon an element.\n",
    "        :param value:\n",
    "        The value upon which the predicate is applied.\n",
    "        \"\"\"\n",
    "        return self.predicate(value)\n",
    "\n",
    "    @staticmethod\n",
    "    def continuous(attribute: FeatureType, point: NumericType) -> 'Pivot':\n",
    "        \"\"\"\n",
    "        A static factory method for building a pivot that operates upon continuous (numerical)\n",
    "        values.\n",
    "        :param attribute:\n",
    "        The name (str) or index (int) that represents the key of the attribute value\n",
    "        upon which the pivot is applied within each element of a set of homogenous elements.\n",
    "        :param point:\n",
    "        The discrete value that represents the pivot point.\n",
    "        :return:\n",
    "        A pivot in the form of: lambda value: value[attribute] <= point\n",
    "        \"\"\"\n",
    "        def predicate(value: any) -> bool:\n",
    "            return value[attribute] <= point\n",
    "        return Pivot(predicate=predicate, info=(attribute, point, '<=', '>'))\n",
    "\n",
    "    def __str__(self, condition: bool = True) -> str:\n",
    "        \"\"\"\n",
    "        The human-intelligible, textual representation of this pivot.\n",
    "        :param condition:\n",
    "        The negation of the predicate, if false.\n",
    "        \"\"\"\n",
    "        operator: str = self.true_condition() if condition else self.false_condition()\n",
    "        return f'x[{self.attribute()}] {operator.ljust(2)} {self.point()}'\n",
    "\n",
    "\n",
    "class NumericalPivotCandidate:\n",
    "    \"\"\"\n",
    "    A component data-structure for tracking the set of parameters that best splits a\n",
    "    continuous attribute.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, feature: FeatureType, gain: float, probe: float):\n",
    "        self.__feature = feature\n",
    "        self.__gain = gain\n",
    "        self.__probe = probe\n",
    "\n",
    "    def feature(self) -> FeatureType:\n",
    "        \"\"\"\n",
    "        The name (str) or index (int) that represents the key of the attribute that is being\n",
    "        used as a feature.\n",
    "        \"\"\"\n",
    "        return require(self.__feature, 'feature')\n",
    "\n",
    "    def gain(self) -> float:\n",
    "        \"\"\"\n",
    "        The gain yielded by this combination of feature and probe value.\n",
    "        \"\"\"\n",
    "        return require(self.__gain, 'gain')\n",
    "\n",
    "    def probe(self) -> float:\n",
    "        \"\"\"\n",
    "        The best probe value tested thus far.\n",
    "        \"\"\"\n",
    "        return require(self.__probe, 'probe')\n",
    "\n",
    "    def update(self, feature: FeatureType, gain: float, probe: float) -> bool:\n",
    "        \"\"\"\n",
    "        Compares a new parameter set with the previously best seen parameter set and\n",
    "        updates the internal state of this data-structure if the new parameter set\n",
    "        yields a better gain.\n",
    "        :param feature:\n",
    "        The feature being evaluated.\n",
    "        :param gain:\n",
    "        The gain produced by this split.\n",
    "        :param probe:\n",
    "        The probe value used in producing this split.\n",
    "        :return:\n",
    "        True if this feature combination yielded a better gain than that yielded\n",
    "        by a previous combination, otherwise, False.\n",
    "        \"\"\"\n",
    "        if gain < self.gain():\n",
    "            return False\n",
    "        self.__feature = feature\n",
    "        self.__gain = gain\n",
    "        self.__probe = probe\n",
    "        return True\n",
    "\n",
    "    @staticmethod\n",
    "    def initial() -> 'NumericalPivotCandidate':\n",
    "        \"\"\"\n",
    "        A static factory method for initialising a default NumericalPivotCandidate data-structure\n",
    "        that has not seen previous parameter combinations.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return NumericalPivotCandidate(feature=0, gain=-math.inf, probe=1.0)\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\"\n",
    "        The human-intelligible, textual representation of the optimal parameter set.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return f'feature: {self.__feature}, gain: {self.__gain}, probe: {self.__probe}'\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Section 3.3 - Node"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "class Node(metaclass=abc.ABCMeta):\n",
    "    \"\"\"\n",
    "    An interface that encapsulates the logic of determining the class membership of an element\n",
    "    within the hierarchy of a decision tree.\n",
    "    \"\"\"\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def eval(self, element: any) -> any:\n",
    "        \"\"\"\n",
    "        Computes the class membership of an element, typically by means of traversing the node hierarchy\n",
    "        recursively.\n",
    "        :param element:\n",
    "        The element whose class we wish to determine.\n",
    "        :return:\n",
    "        The class membership of the given element.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError('Node#eval')\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def depth(self) -> int:\n",
    "        \"\"\"\n",
    "        Computes the number of levels (children) beneath this specific node, inclusive\n",
    "        of the current node.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def prune(self) -> 'Node':\n",
    "        \"\"\"\n",
    "        A mechanism for producing locally optimal or more efficient node configurations\n",
    "        dependent upon a nodes internal state. This method provides no default implementation\n",
    "        as optimisation is not necessary, although desirable, to the proper functioning of\n",
    "        a decision tree.\n",
    "        \"\"\"\n",
    "        return self\n",
    "\n",
    "    @staticmethod\n",
    "    def terminate(value: any) -> 'Node':\n",
    "        \"\"\"\n",
    "        A static factory method for producing a terminal (leaf) node with the given value.\n",
    "        :param value:\n",
    "        The class membership of this terminal node.\n",
    "        \"\"\"\n",
    "        return TerminalNode(value=value)\n",
    "\n",
    "    @staticmethod\n",
    "    def branch(pivot: 'Pivot', true_branch: Optional['Node'] = None, false_branch: Optional['Node'] = None) -> 'Node':\n",
    "        \"\"\"\n",
    "        A static factory method for producing a branch (internal) node with the given pivot and true/false branch nodes.\n",
    "        Used to handle mappings with a single split.\n",
    "        :param pivot:\n",
    "        The predicate upon which elements are pivoted.\n",
    "        :param true_branch:\n",
    "        The branch elements are directed to when the predicate evaluates true.\n",
    "        :param false_branch:\n",
    "        The branch elements are directed to when the predicate evaluates false.\n",
    "        \"\"\"\n",
    "        return BranchNode(pivot=pivot, true_branch=true_branch, false_branch=false_branch)\n",
    "\n",
    "    @staticmethod\n",
    "    def lookup(mapping: Dict[any, 'Node'], feature: FeatureType, default: any) -> 'Node':\n",
    "        \"\"\"\n",
    "        A static factory method for producing a branch (internal) node with the given attribute mappings.\n",
    "        Used to handle mappings with an arbitrary number of splits.\n",
    "        :param mapping:\n",
    "        The dictionary of arbitrary key to node mappings.\n",
    "        :param feature:\n",
    "        The feature whose value is applied as the lookup key against the given mapping.\n",
    "        \"\"\"\n",
    "        return LookupNode(mapping=mapping, feature=feature, default=default)\n",
    "\n",
    "\n",
    "class TerminalNode(Node):\n",
    "    \"\"\"\n",
    "    An implementation of the Node interface that statically resolves the class membership of an element.\n",
    "    Analogous to a leaf node in standard decision tree implementations.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, value: any):\n",
    "        self.__value = require(value, 'value')\n",
    "\n",
    "    def eval(self, element: any) -> any:\n",
    "        \"\"\"\n",
    "        Computes the class membership of the given element by statically mapping it to this node's value.\n",
    "        \"\"\"\n",
    "        return self.value\n",
    "\n",
    "    def depth(self) -> int:\n",
    "        \"\"\"\n",
    "        This implementation has no children, therefore its depth is always 1.\n",
    "        \"\"\"\n",
    "        return 1\n",
    "\n",
    "    @property\n",
    "    def value(self) -> any:\n",
    "        \"\"\"\n",
    "        The class membership value that this node evaluates to.\n",
    "        \"\"\"\n",
    "        return self.__value\n",
    "\n",
    "\n",
    "class BranchNode(Node):\n",
    "    \"\"\"\n",
    "    An implementation of the Node interface that dynamically resolves the class membership of an element.\n",
    "    Analogous to an internal node in standard decision tree implementations.\n",
    "    Used to handle mappings with a single split.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, pivot: 'Pivot', true_branch: 'Node', false_branch: 'Node'):\n",
    "        self.__pivot = require(pivot, 'pivot')\n",
    "        self.__nodes = np.asarray([false_branch, true_branch])\n",
    "\n",
    "    def eval(self, element: any) -> any:\n",
    "        \"\"\"\n",
    "        Dynamically computes the class membership of the given element by mapping it to another branch, by means of\n",
    "        applying the pivot's predicate, and then recursing until arriving at a terminal node.\n",
    "        \"\"\"\n",
    "        # Optimisation: removed the conditional branch to eliminate the cost incurred by miss-predicting a branch.\n",
    "        # previous form: branch: Node = self.true_branch if self.pivot.split(element) else self.false_branch\n",
    "        index: int = int(self.pivot.split(element))\n",
    "        branch: Node = self.nodes[index]\n",
    "        return branch.eval(element)\n",
    "\n",
    "    def depth(self) -> int:\n",
    "        \"\"\"\n",
    "        Computes the depth of this node and its deepest child.\n",
    "        \"\"\"\n",
    "        levels = [child.depth() for child in self.nodes]\n",
    "        return 1 + np.amax(levels, initial=0)\n",
    "\n",
    "    def prune(self) -> 'Node':\n",
    "        \"\"\"\n",
    "        This mechanism attempts to optimise the internal structure of this node.\n",
    "        - eliminates the branch condition if possible.\n",
    "        \"\"\"\n",
    "        if isinstance(self.true_branch, TerminalNode) and isinstance(self.false_branch, TerminalNode):\n",
    "            if self.true_branch.value == self.false_branch.value:\n",
    "                return self.true_branch\n",
    "        return self\n",
    "\n",
    "    @property\n",
    "    def pivot(self) -> 'Pivot':\n",
    "        \"\"\"\n",
    "        The pivot condition of this node.\n",
    "        \"\"\"\n",
    "        return self.__pivot\n",
    "\n",
    "    @property\n",
    "    def nodes(self):\n",
    "        \"\"\"\n",
    "        The array of nodes representing each branch.\n",
    "        Always of length 2 in the form [false_branch, true_branch]\n",
    "        \"\"\"\n",
    "        return self.__nodes\n",
    "\n",
    "    @property\n",
    "    def true_branch(self) -> 'Node':\n",
    "        \"\"\"\n",
    "        The node applied when the pivot condition evaluates truthfully.\n",
    "        \"\"\"\n",
    "        return self.nodes[int(True)]\n",
    "\n",
    "    @property\n",
    "    def false_branch(self) -> 'Node':\n",
    "        \"\"\"\n",
    "        The node applied when the pivot condition evaluates falsely.\n",
    "        \"\"\"\n",
    "        return self.nodes[int(False)]\n",
    "\n",
    "\n",
    "class LookupNode(Node):\n",
    "    \"\"\"\n",
    "    An implementation of the Node interface that dynamically resolves the class membership of an element.\n",
    "    An extended version of the internal node in standard decision tree implementations that is used to handle\n",
    "    mappings with an arbitrary number of splits.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mapping: Dict[any, 'Node'], feature: FeatureType, default: any):\n",
    "        self.__mapping = require(mapping, 'mapping')\n",
    "        self.__feature = require(feature, 'feature')\n",
    "        self.__default = require(default, 'default')\n",
    "\n",
    "    def eval(self, element: any) -> any:\n",
    "        \"\"\"\n",
    "        Dynamically computes the class membership of the given element by mapping it to another branch, by means of\n",
    "        looking up the feature value within the node's mapping, and then recursing until arriving at a terminal node.\n",
    "        \"\"\"\n",
    "        key = element[self.feature]\n",
    "        lookup: Node = self.mapping.get(key, None)\n",
    "        if lookup is None:\n",
    "            return self.default\n",
    "        return lookup.eval(element)\n",
    "\n",
    "    def depth(self, level: int = 0) -> int:\n",
    "        \"\"\"\n",
    "        Computes the depth of this node and its deepest child.\n",
    "        \"\"\"\n",
    "        levels = [child.depth() for child in self.mapping.values()]\n",
    "        return 1 + np.amax(levels, initial=0)\n",
    "\n",
    "    def prune(self) -> 'Node':\n",
    "        \"\"\"\n",
    "        This mechanism attempts to optimise the internal structure of this node.\n",
    "        - eliminates the mapping if there is only one element.\n",
    "        N.B. This type of node could be subject to much further optimisation efforts.\n",
    "        \"\"\"\n",
    "        size = len(self.mapping)\n",
    "        if size == 1:\n",
    "            return next(iter(self.mapping.values()))\n",
    "        return self\n",
    "\n",
    "    @property\n",
    "    def mapping(self) -> Dict[any, 'Node']:\n",
    "        \"\"\"\n",
    "        The internal mapping between feature values and nodes.\n",
    "        \"\"\"\n",
    "        return self.__mapping\n",
    "\n",
    "    @property\n",
    "    def feature(self) -> FeatureType:\n",
    "        \"\"\"\n",
    "        The feature whose values are mapped by this node.\n",
    "        \"\"\"\n",
    "        return self.__feature\n",
    "\n",
    "    @property\n",
    "    def default(self) -> any:\n",
    "        \"\"\"\n",
    "        The default value to return if the feature mapping fails.\n",
    "        \"\"\"\n",
    "        return self.__default\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Section 3.4 - Decision Tree Builders"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "# A mapping of all available implementations of the SplitCriterionMetric interface.\n",
    "metrics: Dict[str, SplitCriterionMetric] = {\n",
    "    'entropy': Entropy(),\n",
    "    'gini': GiniImpurity()\n",
    "}\n",
    "\n",
    "\n",
    "# The default implementation of the SplitCriterionMetric to use in tree construction.\n",
    "DEFAULT_CRITERION: str = 'entropy'\n",
    "\n",
    "\n",
    "def get_metric(name: str) -> SplitCriterionMetric:\n",
    "    \"\"\"\n",
    "    A utility function for safely retrieving a split criterion metric of the given name.\n",
    "    \"\"\"\n",
    "    key: str = name if name in metrics.keys() else DEFAULT_CRITERION\n",
    "    return require(metrics.get(key), f'metric => {name}')\n",
    "\n",
    "\n",
    "class DecisionTreeBuilder(metaclass=abc.ABCMeta):\n",
    "    \"\"\"\n",
    "    An interface that encapsulates the logic of constructing the internal node hierarchy of decision trees.\n",
    "    \"\"\"\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def build(self, x, y, **kwargs) -> 'Node':\n",
    "        \"\"\"\n",
    "        The public method responsible for constructing the node hierarchy according to some node construction\n",
    "        algorithm, i.e., CART, ID3, C4.5, etc.\n",
    "        :param x:\n",
    "        The set of data elements.\n",
    "        :param y:\n",
    "        The set of class membership labels corresponding to each element in the set of data elements.\n",
    "        :param kwargs:\n",
    "        Any additional parameters passed to the node construction algorithm.\n",
    "        :return:\n",
    "        The root node of a decision tree representing the node hierarchy.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError('DecisionTreeBuilder#build')\n",
    "\n",
    "    def compute_metric(self, attributes, criterion: str) -> float:\n",
    "        \"\"\"\n",
    "        A mechanism for computing the split value for the given collection of attributes.\n",
    "        :param attributes:\n",
    "        The set of attribute values.\n",
    "        :param criterion:\n",
    "        The name of the split criterion metric to use.\n",
    "        :return:\n",
    "        The floating point representation of the metric that was computed upon the relative frequencies\n",
    "        of each attribute's value within the set of values.\n",
    "        \"\"\"\n",
    "        _, frequencies = value_counts(attributes, normalise=True)\n",
    "        metric: SplitCriterionMetric = get_metric(criterion)\n",
    "        return metric.compute(frequencies=frequencies)\n",
    "\n",
    "    def information_gain_categorical(self, target, feature, criterion: str) -> float:\n",
    "        \"\"\"\n",
    "        A mechanism for computing the split value for the given collection of categorical attributes.\n",
    "        :param target:\n",
    "        The set of target values.\n",
    "        :param feature:\n",
    "        The set of feature values.\n",
    "        :param criterion:\n",
    "        The name of the split criterion metric to use.\n",
    "        :return:\n",
    "        The information gain yielded by performing the split against this feature.\n",
    "        \"\"\"\n",
    "        total_info = self.compute_metric(target, criterion=criterion)\n",
    "        split_info = self.compute_metric(feature, criterion=criterion)\n",
    "        result: float = total_info - split_info\n",
    "        return result\n",
    "\n",
    "    def information_gain_continuous(self, target, feature, point, criterion: str) -> float:\n",
    "        \"\"\"\n",
    "        A mechanism for computing the split value for the given collection of continuous attributes split at the\n",
    "        given point value.\n",
    "        :param target:\n",
    "        The set of target values.\n",
    "        :param feature:\n",
    "        The set of feature values.\n",
    "        :param point:\n",
    "        The pivot point value.\n",
    "        :param criterion:\n",
    "        The name of the split criterion metric to use.\n",
    "        :return:\n",
    "        The information gain yielded by performing the split against this feature at the given pivot point.\n",
    "        \"\"\"\n",
    "        size, split_indices = len(target), np.asarray([feature <= point, feature > point])\n",
    "        total_info = self.compute_metric(target, criterion=criterion)\n",
    "        split_info = np.sum(np.asarray([(self.compute_metric(feature[index], criterion=criterion) * (np.count_nonzero(index) / size)) for index in split_indices]))\n",
    "        result: float = total_info - split_info\n",
    "        return result\n",
    "\n",
    "    @staticmethod\n",
    "    def factory(implementation: str, **kwargs) -> 'DecisionTreeBuilder':\n",
    "        \"\"\"\n",
    "        A static factory method for constructing an instance of DecisionTreeBuilder.\n",
    "        :param implementation:\n",
    "        The name of the implementation to use, i.e., ID3.\n",
    "        :param kwargs:\n",
    "        A map of arguments to supply the constructor of the DecisionTreeBuilder implementation.\n",
    "        :return:\n",
    "        A constructed instance of DecisionTreeBuilder.\n",
    "        \"\"\"\n",
    "        factories = {\n",
    "            'ID3': ID3DecisionTreeBuilder\n",
    "        }\n",
    "        constructor = require(factories.get(implementation, None), implementation)\n",
    "        return constructor(**kwargs)\n",
    "\n",
    "    @staticmethod\n",
    "    def default() -> 'DecisionTreeBuilder':\n",
    "        \"\"\"\n",
    "        Constructs a reasonably selected default implementation of the DecisionTreeBuilder interface.\n",
    "        Currently the default implementation is ID3.\n",
    "        \"\"\"\n",
    "        return DecisionTreeBuilder.factory('ID3')\n",
    "\n",
    "\n",
    "class ID3DecisionTreeBuilder(DecisionTreeBuilder):\n",
    "    \"\"\"\n",
    "    An implementation of the DecisionTreeBuilder interface that constructs internal node hierarchy of a decision tree\n",
    "    as per the specification of the Iterative Dichotomiser 3 (ID3) algorithm by Ross Quinlan.\n",
    "\n",
    "    The specification can be found at the following location: https://en.wikipedia.org/wiki/ID3_algorithm#Algorithm\n",
    "    \"\"\"\n",
    "    \n",
    "    def build(self, x, y, criterion: str = DEFAULT_CRITERION) -> 'Node':\n",
    "        \"\"\"\n",
    "        The public API for constructing the ID3 node hierarchy for the given set of elements and class membership labels.\n",
    "        :param x:\n",
    "        The set of data elements.\n",
    "        :param y:\n",
    "        The set of class membership labels.\n",
    "        :param criterion:\n",
    "        The name of the split criterion metric to use.\n",
    "        :return:\n",
    "        The constructed node hierarchy.\n",
    "        \"\"\"\n",
    "        data = x.copy()\n",
    "        data[y.name] = y\n",
    "        return self._build(data=data, features=x.columns, target=y.name, criterion=criterion)\n",
    "\n",
    "    def _build(self, data, features, target: FeatureType, criterion: str, parent_class=None) -> 'Node':\n",
    "        \"\"\"\n",
    "        The internal API for constructing the ID3 node hierarchy.\n",
    "        :param data:\n",
    "        The set of elements and target values.\n",
    "        :param features:\n",
    "        The set of features.\n",
    "        :param target:\n",
    "        The target attribute to predict.\n",
    "        :param criterion:\n",
    "        The name of the split criterion metric to use.\n",
    "        :param parent_class:\n",
    "        The majority class of the previous node (if any).\n",
    "        :return:\n",
    "        The constructed ID3 node hierarchy.\n",
    "        \"\"\"\n",
    "        target_subset = data[target]\n",
    "        classes = np.unique(target_subset)\n",
    "        choices = len(classes)\n",
    "\n",
    "        # base case #1\n",
    "        # There are no examples in the subset, which happens when no example in the parent set was found to match a\n",
    "        # specific value of the selected attribute.\n",
    "        if len(data) <= 0:\n",
    "            return Node.terminate(parent_class)\n",
    "\n",
    "        # base case #2\n",
    "        # Every element of the subset belongs to the same class.\n",
    "        if choices <= 1:\n",
    "            return Node.terminate(classes[0])\n",
    "\n",
    "        majority_class = classes[majority_class_index(elements=target_subset)]\n",
    "\n",
    "        # base case #3\n",
    "        # There are no more attributes to be selected, but the examples still do not belong to the same class.\n",
    "        if len(features) <= 0:\n",
    "            return Node.terminate(majority_class)\n",
    "\n",
    "        gains = np.asarray([self.information_gain_categorical(target=target_subset, feature=data[feature], criterion=criterion) for feature in features])\n",
    "        best_feature = features[np.argmax(gains)]\n",
    "        attribute = data[best_feature]\n",
    "        remaining_features = [feature for feature in features if feature != best_feature]\n",
    "\n",
    "        try:\n",
    "            # attempt to treat the best feature as a numerical attribute\n",
    "            subtree: Node = self._build_continuous(data=data, features=remaining_features, target=target, criterion=criterion, parent_class=majority_class, best_feature=best_feature, attribute=attribute)\n",
    "        except Exception as e:\n",
    "            # fallback to treating the best feature as a categorical attribute\n",
    "            subtree: Node = self._build_categorical(data=data, features=remaining_features, target=target, criterion=criterion, parent_class=majority_class, best_feature=best_feature, attribute=attribute)\n",
    "\n",
    "        # locally optimise and return the pruned subtree\n",
    "        return self._prune(subtree)\n",
    "\n",
    "    def _prune(self, node: Node) -> 'Node':\n",
    "        \"\"\"\n",
    "        Attempts to reduce the complexity of the node hierarchy according to local optimisation\n",
    "        heuristics.\n",
    "        :param node:\n",
    "        The node to prune.\n",
    "        :return:\n",
    "        The most-pruned node.\n",
    "        \"\"\"\n",
    "        # more elegantly expresses the below in python 3.8+\n",
    "        # while (pruned := node.prune()) != node:\n",
    "        #     node = pruned\n",
    "\n",
    "        pruned: Node = node.prune()\n",
    "        while pruned != node:\n",
    "            node = pruned\n",
    "            pruned = node.prune()\n",
    "\n",
    "        return node\n",
    "\n",
    "    def _build_continuous(self, data, features, target, criterion: str, parent_class, best_feature, attribute) -> 'Node':\n",
    "        \"\"\"\n",
    "        A mechanism for building a node that attempts to find the best split of a given numerical attribute.\n",
    "        :param data:\n",
    "        The set of data elements.\n",
    "        :param features:\n",
    "        The set of feature names.\n",
    "        :param target:\n",
    "        The target attribute to predict.\n",
    "        :param criterion:\n",
    "        The name of the split criterion metric to use.\n",
    "        :param parent_class:\n",
    "        The best class membership of the set of data elements evaluated immediately prior to this.\n",
    "        :param best_feature:\n",
    "        The name of the best feature.\n",
    "        :param attribute:\n",
    "        The best feature data element.\n",
    "        \"\"\"\n",
    "        # create probing values for use as the pivot point of the binary split\n",
    "        probes = self.create_probe_values(attribute.min(), attribute.max())\n",
    "        # keep track of the best parameter combination\n",
    "        candidate: NumericalPivotCandidate = NumericalPivotCandidate.initial()\n",
    "        for probe in probes:\n",
    "            # evaluate each pivot point candidate\n",
    "            gain = self.information_gain_continuous(target=data[target], feature=attribute, point=probe, criterion=criterion)\n",
    "            candidate.update(feature=best_feature, gain=gain, probe=probe)\n",
    "\n",
    "        def build_subtree(indices) -> 'Node':\n",
    "            \"\"\"\n",
    "            Utility function for recursively building the tree node hierarchy for a given set of indices.\n",
    "            \"\"\"\n",
    "            return self._build(data=data[indices], features=features, target=target, criterion=criterion, parent_class=parent_class)\n",
    "\n",
    "        # TODO: refactor below\n",
    "        # create the pivot point and the branches on each side of the pivot\n",
    "        pivot: Pivot = Pivot.continuous(candidate.feature(), candidate.probe())\n",
    "        true_branch = build_subtree(data[candidate.feature()] <= candidate.probe())\n",
    "        false_branch = build_subtree(data[candidate.feature()] > candidate.probe())\n",
    "\n",
    "        return Node.branch(pivot=pivot, true_branch=true_branch, false_branch=false_branch)\n",
    "\n",
    "    def _build_categorical(self, data, features, target, criterion: str, parent_class, best_feature, attribute) -> 'Node':\n",
    "        \"\"\"\n",
    "        A mechanism for building a node that attempts to find the best split for all values of a given categorical attribute.\n",
    "        :param data:\n",
    "        The set of data elements.\n",
    "        :param features:\n",
    "        The set of feature names.\n",
    "        :param target:\n",
    "        The target attribute to predict.\n",
    "        :param criterion:\n",
    "        The name of the split criterion metric to use.\n",
    "        :param parent_class:\n",
    "        The best class membership of the set of data elements evaluated immediately prior to this.\n",
    "        :param best_feature:\n",
    "        The name of the best feature.\n",
    "        :param attribute:\n",
    "        The best feature data element.\n",
    "        \"\"\"\n",
    "        values, counts = value_counts(attribute, normalise=False)\n",
    "        # assign the most common class as being the default/fallback value\n",
    "        fallback = values[np.argmax(counts)]\n",
    "        mapping: Dict[any, Node] = {}\n",
    "\n",
    "        for value in values:\n",
    "            # iterate over each seen value and recursively build a subtree for the given split\n",
    "            mapping[value] = self._build(data=data.where(attribute == value).dropna(), features=features, target=target, criterion=criterion, parent_class=parent_class)\n",
    "\n",
    "        return Node.lookup(mapping=mapping, feature=best_feature, default=fallback)\n",
    "\n",
    "    def create_probe_values(self, minima, maxima):\n",
    "        \"\"\"\n",
    "        A mechanism for generating a set of candidate pivot point values given the min/max\n",
    "        values of a numerical attribute.\n",
    "        \"\"\"\n",
    "        weights = np.arange(0.0, 1.0, 0.05)\n",
    "        return np.asarray([point * minima + (1.0 - point) * maxima for point in weights])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Section 3.5 - Model Implementations"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Model(metaclass=abc.ABCMeta):\n",
    "    \"\"\"\n",
    "    An interface that encapsulates the common functionality of a machine learning model.\n",
    "    \"\"\"\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def fit(self, x, y, **kwargs):\n",
    "        \"\"\"\n",
    "        A mechanism for training an implementation of this interface on the given\n",
    "        data and class membership elements.\n",
    "        Must be implemented by inheritors of this interface.\n",
    "        :param x:\n",
    "        The set of data elements.\n",
    "        :param y:\n",
    "        The set of class membership labels corresponding to each element in the set of data elements.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError('Model#fit')\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def predict(self, x, **kwargs):\n",
    "        \"\"\"\n",
    "        A mechanism for predicting the class membership of data elements in a given set.\n",
    "        Must be implemented by inheritors of this interface.\n",
    "        :param x:\n",
    "        The set of data elements.\n",
    "        :return:\n",
    "        The predicted class membership labels for each element in the set of data elements.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError('Model#predict')\n",
    "\n",
    "    def compile(self, **kwargs):\n",
    "        \"\"\"\n",
    "        A mechanism for (re)configuring the parameters of an implementation of\n",
    "        this interface.\n",
    "        The default implementation is intentionally a no-op.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class DecisionTree(Model):\n",
    "    \"\"\"\n",
    "    An implementation of the Model interface that provides support for decision trees.\n",
    "    \"\"\"\n",
    "\n",
    "    __builder: 'DecisionTreeBuilder' = DecisionTreeBuilder.default()\n",
    "    __root: Optional['Node'] = None\n",
    "    __depth: Optional[int] = None\n",
    "\n",
    "    def compile(self, **kwargs):\n",
    "        \"\"\"\n",
    "        A mechanism for (re)configuring the DecisionTreeBuilder employed by this decision tree\n",
    "        model.\n",
    "        \"\"\"\n",
    "        previous: DecisionTreeBuilder = self.__builder\n",
    "        try:\n",
    "            # attempt to instantiate the request implementation\n",
    "            builder = DecisionTreeBuilder.factory(kwargs['implementation'], **kwargs)\n",
    "        except (KeyError, ValueError):\n",
    "            # do not change the internal state in the case of an 'expected' error.\n",
    "            builder = previous\n",
    "        self.__builder = builder\n",
    "\n",
    "    def fit(self, x, y, **kwargs):\n",
    "        \"\"\"\n",
    "        Trains this decision tree by constructing its node hierarchy. Delegates construction\n",
    "        to the selected DecisionTreeBuilder implementation.\n",
    "        :param x:\n",
    "        The set of data elements.\n",
    "        :param y:\n",
    "        The set of class membership labels corresponding to each element in the set of data elements.\n",
    "        \"\"\"\n",
    "        # construct the decision tree's node hierarchy\n",
    "        self.__root = self.builder.build(x, y, **kwargs)\n",
    "        # nullify the cached depth\n",
    "        self.__depth = None\n",
    "\n",
    "    def predict(self, x, **kwargs):\n",
    "        \"\"\"\n",
    "        Predicts the class membership of the given data elements by traversing through this tree's node hierarchy.\n",
    "        :param x:\n",
    "        The set of data elements.\n",
    "        :return:\n",
    "        The predicted class membership labels for each element in the set of data elements.\n",
    "        \"\"\"\n",
    "        tree: Node = self.root\n",
    "        samples = x.to_dict(orient='records')\n",
    "        return np.asarray([tree.eval(sample) for sample in samples])\n",
    "\n",
    "    @property\n",
    "    def builder(self) -> 'DecisionTreeBuilder':\n",
    "        \"\"\"\n",
    "        The DecisionTreeBuilder internally used to construct the decision tree's node hierarchy.\n",
    "        \"\"\"\n",
    "        return require(self.__builder, 'builder')\n",
    "\n",
    "    @property\n",
    "    def root(self) -> 'Node':\n",
    "        \"\"\"\n",
    "        The root node of this decision tree.\n",
    "        \"\"\"\n",
    "        return require(self.__root, 'root')\n",
    "\n",
    "    def depth(self) -> int:\n",
    "        \"\"\"\n",
    "        The depth of this decision tree.\n",
    "        This value is computed lazily in for the sake of efficiency.\n",
    "        \"\"\"\n",
    "        depth: int\n",
    "        if self.__root is None:\n",
    "            # the tree has not been built.\n",
    "            depth = 0\n",
    "        elif self.__depth is not None:\n",
    "            # the depth is cached as it has previously been calculated and the tree has not changed.\n",
    "            depth = self.__depth\n",
    "        else:\n",
    "            # the depth has not been calculated prior to this invocation.\n",
    "            depth = self.__depth = self.root.depth()\n",
    "        return depth"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Section 4 - Implementation\n",
    "The \"Business Logic\" of this application."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- RESULTS -- iris -- entropy ---\n",
      "Our Accuracy:   0.9111111111111111\n",
      "Their Accuracy: 1.0\n",
      "Differences: [ 3 32 35 42]\n",
      "Ours:        [2 2 1 2]\n",
      "Theirs:      [1 1 2 1]\n",
      "Correct:     [1 1 2 1]\n",
      "\n",
      "--- RESULTS -- iris -- gini ---\n",
      "Our Accuracy:   0.7555555555555555\n",
      "Their Accuracy: 1.0\n",
      "Differences: [ 0  1  4  5  7 10 18 21 25 31 43]\n",
      "Ours:        [2 1 2 1 1 1 2 1 1 1 1]\n",
      "Theirs:      [1 0 1 0 2 2 1 2 2 0 0]\n",
      "Correct:     [1 0 1 0 2 2 1 2 2 0 0]\n",
      "\n",
      "--- RESULTS -- wine -- entropy ---\n",
      "Our Accuracy:   0.8333333333333334\n",
      "Their Accuracy: 0.8148148148148148\n",
      "Differences: [ 1  2  9 12 28 33 36 43 44 50 53]\n",
      "Ours:        [2 2 2 2 2 1 2 2 2 1 2]\n",
      "Theirs:      [0 0 0 0 1 0 1 1 1 0 1]\n",
      "Correct:     [0 2 2 0 1 0 2 1 2 1 2]\n",
      "\n",
      "--- RESULTS -- wine -- gini ---\n",
      "Our Accuracy:   0.7777777777777778\n",
      "Their Accuracy: 0.9444444444444444\n",
      "Differences: [ 6  8  9 10 11 12 13 21 36 39 40 42 43 44 50]\n",
      "Ours:        [0 0 0 0 2 1 2 0 0 0 0 0 0 0 0]\n",
      "Theirs:      [1 1 2 1 0 0 1 1 2 2 1 1 1 2 1]\n",
      "Correct:     [1 1 2 0 2 0 1 1 2 2 0 1 1 2 1]\n",
      "\n",
      "--- RESULTS -- junk -- entropy ---\n",
      "Our Accuracy:   0.4\n",
      "Their Accuracy: Exception occurred during processing: could not convert string to float: 'W'\n",
      "Differences: N/A\n",
      "Ours:        N/A\n",
      "Theirs:      N/A\n",
      "Correct:     N/A\n",
      "\n",
      "--- RESULTS -- junk -- gini ---\n",
      "Our Accuracy:   0.4\n",
      "Their Accuracy: Exception occurred during processing: could not convert string to float: 'W'\n",
      "Differences: N/A\n",
      "Ours:        N/A\n",
      "Theirs:      N/A\n",
      "Correct:     N/A\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def debug_tree(node, depth: int = 0):\n",
    "    \"\"\"\n",
    "    A utility function for outputting the structural representation of a decision tree\n",
    "    implemented within this package or sklearn.\n",
    "    \"\"\"\n",
    "    if isinstance(node, DecisionTreeClassifier):\n",
    "        print(export_text(node))\n",
    "        return\n",
    "    if isinstance(node, DecisionTree):\n",
    "        debug_tree(node.root)\n",
    "        return\n",
    "\n",
    "    padding = ('|   ' * depth)[:-3] + '---'\n",
    "\n",
    "    def p(o):\n",
    "        print(f'{str(depth).ljust(2)} {padding} {o}')\n",
    "\n",
    "    if isinstance(node, TerminalNode):\n",
    "        p(f'class: {node.value}')\n",
    "    elif isinstance(node, BranchNode):\n",
    "        p(f'{node.pivot.__str__(condition=True)}')\n",
    "        debug_tree(node.true_branch, depth=depth+1,)\n",
    "        p(f'{node.pivot.__str__(condition=False)}')\n",
    "        debug_tree(node.false_branch, depth=depth+1)\n",
    "    elif isinstance(node, LookupNode):\n",
    "        for (k, v) in node.mapping.items():\n",
    "            p(f'lookup: {node.feature} {k}')\n",
    "            debug_tree(v, depth=depth+1)\n",
    "    else:\n",
    "        raise ValueError(f'Unexpected node: {node}')\n",
    "\n",
    "\n",
    "def junk_mixed_dataset(samples: int = 100):\n",
    "    \"\"\"\n",
    "    A utility function that constructs a junk dataset that contains categorical (and numerical) attributes\n",
    "    to illustrate that the decision tree implemented within this package is capable of operating upon\n",
    "    both types of attributes.\n",
    "    \"\"\"\n",
    "    attribute_pool = {\n",
    "        'wind_direction': ['N', 'S', 'E', 'W'],\n",
    "        'tide': ['Low', 'High'],\n",
    "        'swell_forecasting': ['small', 'medium', 'large'],\n",
    "        'good_waves': ['Yes', 'No'],\n",
    "        'temp': '',\n",
    "        'hello': ''\n",
    "    }\n",
    "    df = pd.DataFrame(columns=attribute_pool.keys())\n",
    "    np.random.seed(42)\n",
    "    for i in range(samples):\n",
    "        df.loc[i, 'wind_direction'] = str(np.random.choice(attribute_pool['wind_direction'], 1)[0]) # categorical\n",
    "        df.loc[i, 'tide'] = str(np.random.choice(attribute_pool['tide'], 1)[0]) # categorical\n",
    "        df.loc[i, 'swell_forecasting'] = str(np.random.choice(attribute_pool['swell_forecasting'], 1)[0]) # categorical\n",
    "        df.loc[i, 'good_waves'] = str(np.random.choice(attribute_pool['good_waves'], 1)[0]) # categorical\n",
    "        df.loc[i, 'good_waves_int'] = int(np.random.choice([0, 1], 1)[0]) # numeric somewhat correlated\n",
    "        df.loc[i, 'temp'] = int(np.random.random() * 26) + 1 # numeric but irrelevant\n",
    "        df.loc[i, 'hello'] = 'world' # categorical but irrelevant\n",
    "\n",
    "    target_attr = df['good_waves'].copy()\n",
    "    df.drop(labels=['good_waves'], axis=1, inplace=True)\n",
    "    return df, target_attr\n",
    "\n",
    "# the datasets to evaluate\n",
    "datasets = {\n",
    "    'iris': lambda: load_iris(return_X_y=True, as_frame=True),\n",
    "    'wine': lambda: load_wine(return_X_y=True, as_frame=True),\n",
    "    'junk': lambda: junk_mixed_dataset(150)\n",
    "}\n",
    "\n",
    "# iterate over each dataset\n",
    "for (name, provider) in datasets.items():\n",
    "    [elements, labels] = provider()\n",
    "    # for deterministic non-determinism\n",
    "    random_state: int = 42\n",
    "    # reserve 70% of the samples for training and leave the remaining 30% unseen for evaluating the model's classification accuracy\n",
    "    train_split_size: float = 0.7\n",
    "    # a feature flag for toggling whether or not to display debug output when running this program\n",
    "    debug: bool = False\n",
    "\n",
    "    # evaluate each split criterion metric\n",
    "    for metric in ['entropy', 'gini']:\n",
    "\n",
    "        if debug:\n",
    "            print(f'--- DATASET -- {name} -- {metric} ---')\n",
    "            print(f'Elements:\\n\\n{elements[:5]}\\n')\n",
    "            print(f'Targets:\\n\\n{labels[:5]}\\n')\n",
    "\n",
    "        # construct training/testing data elements\n",
    "        train_x, test_x = train_test_split(elements, train_size=train_split_size, random_state=random_state)\n",
    "        # construct training/testing class membership labels\n",
    "        train_y, test_y = train_test_split(labels, train_size=train_split_size, random_state=random_state)\n",
    "\n",
    "        # construct an implementation of our decision tree model\n",
    "        ours: DecisionTree = DecisionTree()\n",
    "        # construct an implementation of sklearns decision tree classifier\n",
    "        theirs: DecisionTreeClassifier = DecisionTreeClassifier(criterion=metric)\n",
    "\n",
    "        def evaluate(text: str, classifier):\n",
    "            \"\"\"\n",
    "            A utlity function for evaluating this package and sklearns decision tree models\n",
    "            in a consistent manner.\n",
    "            \"\"\"\n",
    "            try:\n",
    "                if isinstance(classifier, DecisionTree):\n",
    "                    classifier.fit(train_x, train_y, criterion=metric)\n",
    "                else:\n",
    "                    classifier.fit(train_x, train_y)\n",
    "\n",
    "                if debug:\n",
    "                    print()\n",
    "                    print(f'--- {text} ---')\n",
    "                    debug_tree(classifier)\n",
    "                    print(f'--- {text} ---')\n",
    "                    print()\n",
    "\n",
    "                predictions = classifier.predict(test_x)\n",
    "                accuracy = accuracy_score(test_y, predictions)\n",
    "                return accuracy, predictions\n",
    "            except Exception as e:\n",
    "                return f'Exception occurred during processing: {e}', None\n",
    "\n",
    "        # our accuracy score and predications\n",
    "        us = evaluate('ours', ours)\n",
    "        # sklearns accuracy score and predications\n",
    "        them = evaluate('theirs', theirs)\n",
    "\n",
    "        if them[1] is not None:\n",
    "            differences = np.nonzero(us[1] - them[1])\n",
    "        else:\n",
    "            differences = None\n",
    "\n",
    "        print(f'--- RESULTS -- {name} -- {metric} ---')\n",
    "        print(f'Our Accuracy:   {us[0]}')\n",
    "        print(f'Their Accuracy: {them[0]}')\n",
    "        print(f'Differences: {np.asarray(differences)[0] if them[1] is not None else \"N/A\"}')\n",
    "        print(f'Ours:        {us[1][differences] if them[1] is not None else \"N/A\"}')\n",
    "        print(f'Theirs:      {them[1][differences] if them[1] is not None else \"N/A\"}')\n",
    "        print(f'Correct:     {np.asarray(test_y)[differences] if them[1] is not None else \"N/A\"}')\n",
    "        print()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}